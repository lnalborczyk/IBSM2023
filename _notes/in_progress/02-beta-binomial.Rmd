# Modèle beta-binomial {#beta-binomial}

```{r setup-ch2, include = FALSE, message = FALSE}
library(tidyverse)
library(knitr)

# setting up knitr options
opts_chunk$set(
  cache = TRUE, echo = TRUE, warning = FALSE, message = FALSE,
  fig.align = "center", out.width = "75%", fig.pos = "!htb"
  )
```

\definecolor{steelblue}{RGB}{70, 130, 180}
\definecolor{green}{RGB}{0, 153, 0}
\definecolor{purple}{RGB}{153, 0, 153}
\definecolor{orangered}{cmyk}{0, 73, 100, 0}

Introduction au chapitre blah blah...

## Coefficient binomial

```{definition, label = "fonction-factorielle", name = "Fonction factorielle"}
On appelle fonction factorielle la fonction qui à tout entier naturel $n$ associe l'entier :

$$N! = N \times (N - 1) \times (N - 2) \times \cdots \times 3 \times 2 \times 1.$$
```

```{definition, label = "coefficient-binomial", name = "Coefficient binomial"}
For any nonnegative integers $k$ and $n$, the binomial coefficient...
```

```{theorem, label = "coefficient-binomial-formule", name = "Formule du coefficient binomial"}
For $k \leq n$, we have:

$$
\left(\begin{array}{l}n \\ k\end{array}\right)=\frac{n(n-1) \cdots(n-k+1)}{k !}=\frac{n !}{(n-k) ! k !}
$$
For $k > n$, we have $\left(\begin{array}{l}n \\ k\end{array}\right) = 0$.
```

```{block, type = "note"}
Blah blah blah...
```

## Le modèle Beta-Binomial

Pourquoi ce modèle ?

Le modèle Beta-Binomial couvre un grand nombre de problèmes de la vie courante :

    - Réussite / échec à un test
    - Présence / absence d'effets secondaires lors du test d'un médicament
    - Résultat d'un questionnaire à réponse binaire vrai / faux 
    - Estimation des résultats du deuxième tour de l'élection présidentielle 

C'est un modèle simple
    
    - Un seul paramètre
    - Solution analytique

### Loi de Bernoulli

S'applique à toutes les situations où le processus de génération des données ne peut résulter qu'en deux issues mutuellement exclusives (e.g., un lancer de pièce). À chaque essai, si on admet que $\Pr(\text{face}) = \theta$, alors $\Pr(\text{pile}) = 1 - \theta$.

Depuis Bernoulli, on sait calculer la probabilité du résultat d'un lancer de pièce, du moment que l'on connait le biais de la pièce $\theta$. Admettons que $Y = 0$ lorsqu'on obtient pile, et que $Y = 1$ lorsqu'on obtient face. Alors $Y$ est distribuée selon une loi de Bernoulli :

$$p(y) = \Pr(Y = y \ | \ \theta) = \theta^{y} (1 - \theta)^{(1 - y)}$$

En remplacant $y$ par $0$ ou $1$, on retombe bien sur nos observations précédentes :

$$\Pr(Y = 0 \ | \ \theta) = \theta^{0} (1 - \theta)^{(1 - 0)} = 1 \times (1 - \theta) = 1 - \theta$$

$$\Pr(Y = 1 \ | \ \theta) = \theta^{1} (1 - \theta)^{(1 - 1)} = \theta \times 1 = \theta$$

### Processus de Bernoulli

Si l'on dispose d'une suite de lancers $\{Y_i\}$ indépendants et identiquement distribués (i.e., chaque lancer a une distribution de Bernoulli de probabilité $\theta$), l'ensemble de ces lancers peut être décrit par une **distribution binomiale**.

Par exemple, imaginons que l'on dispose de la séquence de cinq lancers suivants : Pile, Pile, Pile, Face, Face. On peut recoder cette séquence en $\{0, 0, 0, 1, 1\}$.

Rappel : La probabilité de chaque $1$ est $\theta$ est la probabilité de chaque $0$ est $1 - \theta$.

Quelle est la probabilité d'obtenir 2 faces sur 5 lancers ?

### Processus de Bernoulli

Sachant que les essais sont indépendants les uns des autres, la probabilité d'obtenir cette séquence est de $(1 - \theta) \times (1 - \theta) \times (1 - \theta) \times \theta \times \theta$, c'est à dire : $\theta^{2} (1 - \theta)^{3}$.

On peut généraliser ce résultat pour une séquence de $n$ lancers et $y$ "succès" :

$$\theta^{y} (1 - \theta)^{n - y}$$

Mais, jusque là on a considéré seulement une seule séquence résultant en 2 succès pour 5 lancers, mais il existe de nombreuses séquences pouvant résulter en 2 succès pour 5 lancers (e.g., $\{0, 0, 1, 0, 1\}$)...

### Coefficient binomial

Le **coefficient binomial** nous permet de calculer le nombre d'arrangements possibles résultant en $y$ succès pour $n$ lancers de la manière suivante :

$$
\left(\begin{array}{l} n \\ y \end{array}\right) = C_{n}^{y} = \frac{n !}{y !(n - y) !}
$$

Par exemple pour $y = 1$ et $n = 3$, on sait qu'il existe 3 arrangements possibles : $\{0, 0, 1\}, \{0, 1, 0\}, \{1, 0, 0\}$. On peut vérifier ça par le calcul, en appliquant la formule ci-dessus.

$$
\left(\begin{array}{l} 3 \\ 1\end{array}\right) = C_{1}^{3} = \frac{3 !}{1 !(3 - 1) !} = \frac{6}{2} = 3
$$

```{r binomialcoef} 
# computing the total number of possible arrangements in R
choose(n = 3, k = 1)
```

### Loi binomiale

$$
p(y \ | \ \theta) = \Pr(Y = y \ | \ \theta) = \left(\begin{array}{l} n \\ y \end{array}\right) \theta^{y}(1 - \theta)^{n - y}
$$

La loi binomiale nous permet de calculer la probabilité d'obtenir $y$ succès sur $n$ essais, pour un $\theta$ donné. Exemple de la distribution binomiale pour une pièce non biaisée ($\theta = 0.5$), indiquant la probabilité d'obtenir $n$ faces sur 10 lancers (en R: `dbinom(x = 0:10, size = 10, prob = 0.5)`). 

```{r binomial-barplot2, echo = FALSE, fig.width = 7.5, fig.height = 5, fig.cap = "Binomial distribution barplot..."}
coin <- dbinom(x = 0:10, size = 10, prob = 0.5)

barplot(
  coin, names.arg = 0:10, border = NA, axes = FALSE,
  cex.names = 1.5, col = "grey20"
  )
```

### Générer des données à partir d'une distribution binomiale

```{r berndata, fig.width = 10, fig.height = 5, fig.cap = "Long-run..."}
library(tidyverse)
set.seed(666) # for reproducibility

rbinom(n = 500, size = 1, prob = 0.6) %>% # theta = 0.6
        data.frame %>%
        mutate(x = seq_along(.), y = cumsum(.) / seq_along(.) ) %>%
        ggplot(aes(x = x, y = y), log = "y") +
        geom_line(lwd = 1) +
        geom_hline(yintercept = 0.5, lty = 3) +
        xlab("Nombre de lancers") +
        ylab("Proportion de faces") +
        ylim(0, 1) +
        theme_bw(base_size = 18)
```

### Définition du modèle (likelihood)

#### Fonction de vraisemblance (likelihood)

- Nous considérons $y$ comme étant le nombre de succès
- Nous considérons le nombre d'observations $n$ comme étant une **constante**
- Nous considérons $\theta$ comme étant le **paramètre** de notre modèle (i.e., la probabilité de succès)

La fonction de vraisemblance s'écrit de la manière suivante :

$$
\color{orangered}{\mathcal{L}(\theta\ |\ y, n) = p(y \ |\ \theta, n) = \left(\begin{array}{l} n \\ y \end{array}\right) \theta^{y}(1 - \theta)^{n - y} \propto \theta^{y}(1 - \theta)^{n - y}}
$$

### Vraisemblance versus probabilité

On lance à nouveau une pièce de biais $\theta$ (où $\theta$ représente la probabilité d'obtenir Face). On lance cette pièce deux fois et on obtient une Face et un Pile.

On peut calculer la probabilité de ces données selon (i.e., *en fonction de*) différentes valeurs de $\theta$ de la manière suivante :

$$
\begin{aligned}
\Pr(F, P \ | \ \theta) + \Pr(P, F \ | \ \theta) &= \theta(1 - \theta) + \theta(1 - \theta) \\
&= 2 \times \Pr(P \ | \ \theta) \times \Pr(F \ | \ \theta) \\
&= 2 \theta(1 - \theta)
\end{aligned}
$$

Cette probabilité est définie pour un jeu de données fixe et une valeur de $\theta$ variable. On peut représenter cette fonction visuellement. Représentation graphique de la fonction de vraisemblance de theta pour x = 1 et n = 2...

```{r likelihood, fig.width = 10, fig.height = 5, fig.cap = "Likelihood plot..."}
y <- 1 # number of heads
n <- 2 # number of trials

data.frame(theta = seq(from = 0, to = 1, length.out = 1e3) ) %>%
  mutate(likelihood = dbinom(x = y, size = n, prob = theta) ) %>%
  ggplot(aes(x = theta, y = likelihood) ) +
  geom_area(color = "orangered", fill = "orangered", alpha = 0.5) +
  xlab(expression(paste(theta, " - Pr(face)") ) ) + ylab("Likelihod") +
  theme_bw(base_size = 20)
```

Si on calcule l'aire sous la courbe de cette fonction, on obtient :

$$\int_{0}^{1} 2 \theta(1 - \theta) \mathrm{d} \theta = \frac{1}{3}$$

```{r integral-likelihood}
f <- function(theta) {2 * theta * (1 - theta) }
integrate(f = f, lower = 0, upper = 1)
```

Quand on varie $\theta$, la fonction de vraisemblance *n'est pas* une distribution de probabilité valide (i.e., son intégrale n'est pas égale à 1). On utilise le terme de **vraisemblance**, pour distinguer ce type de fonction des fonctions de densité de probabilité. On utilise la notation suivante pour mettre l'accent sur le fait que la fonction de vraisemblance est une fonction de $\theta$, et que les données sont fixes : $\mathcal{L}(\theta \ | \ data) = p(data \ | \ \theta)$.

```{r likelihood-table, echo = FALSE, eval = knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results = "asis"}
library(kableExtra)
library(knitr)

data.frame(
  theta = as.character(c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0) ),
  x0 = c(1.0, 0.64, 0.36, 0.16, 0.04, 0.00),
  x1 = c(0.0, 0.32, 0.48, 0.48, 0.32, 0.00),
  x2 = c(0.0, 0.04, 0.16, 0.36, 0.64, 1.00)
  ) %>%
  mutate(Total = rowSums(.[2:4]) %>% as.character) %>%
  tibble::add_row(
    theta = "Total", x0 = sum(.$x0), x1 = sum(.$x1), x2 = sum(.$x2),
    Total = ""
    ) %>%
  kable(
    col.names = c("theta", "0", "1", "2", "Total"),
    format = "html",
    align = c("c", "c", "c", "c", "c"),
    caption = "Vraisemblance versus probabilité pour deux lancers de pièce",
    escape = FALSE
    ) %>%
  kable_styling(full_width = TRUE, position = "center") %>%
  add_header_above(c(" " = 1, "Nombre de Faces (y)" = 3, " " = 1) )
```

Notons que la vraisemblance de $\theta$ pour une donnée particulière est égale à la probabilité de cette donnée pour cette valeur de $\theta$. Cependant, la *distribution* de ces vraisemblances (en colonne) n'est pas une distribution de probabilités. Dans l'analyse bayésienne, **les données sont considérées comme fixes** et la valeur de $\theta$ est considérée comme une **variable aléatoire**.

### Définition du prior

Comment définir un prior dans le cas du lancer de pièce ?

**Aspect sémantique** $~\rightarrow~$ *doit pouvoir rendre compte* : 
+ D'une absence d'information
+ D'une connaissance d'observations antérieures concernant la pièce étudiée
+ D'un niveau d'incertitude concernant ces observations antérieures 

**Aspect mathématique** $~\rightarrow~$ *pour une solution entièrement analytique* :
+ Les distributions a priori et a posteriori doivent avoir la même forme
+ La vraisemblance marginale doit pouvoir se calculer analytiquement 

### La distribution Beta

$$
\begin{aligned}
\color{steelblue}{p(\theta\ | \ a, b)} \ &\color{steelblue}{= \mathrm{Beta}(\theta\ |\ a, b)} \\
& \color{steelblue}{= \theta^{a - 1}(1 - \theta)^{b - 1} / B(a, b)} \\
& \color{steelblue}{\propto \theta^{a - 1}(1 - \theta)^{b - 1}}
\end{aligned}
$$

où $a$ et $b$ sont deux paramètres tels que $a \geq 0$, $b \geq 0$, et $B(a, b)$ est une constante de normalisation.

```{r beta1, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Distribution Beta..."}
p <- seq(0, 1, length = 100)

data.frame(
  p = p,
  p1 = dbeta(p, 1, 1),
  p2 = dbeta(p, 2, 2),
  p3 = dbeta(p, 4, 2),
  p4 = dbeta(p, 2, 4)
  ) %>%
  pivot_longer(p1:p4, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = p, y = prob / sum(prob), color = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.6) +
  # geom_line(color = "black", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_discrete(
    name = "Parameters",
    labels = c(
      "a = 1, b = 1",
      "a = 2, b = 2",
      "a = 4, b = 2",
      "a = 2, b = 4"
      )
    )
```

### Interprétation des paramètres du prior Beta

+ On peut exprimer l'absence de connaissance a priori par $a = b = 1$ (distribution orange)
+ On peut exprimer un prior en faveur d'une absence de biais par $a = b > 2$ (distribution verte)
+ On peut exprimer un biais en faveur de *Face* par $a > b$ (distribution bleue)
+ On peut exprimer un biais en faveur de *Pile* par $a < b$ (distribution violette)

```{r beta2, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Inteprétation des paramètres d'une distribution Beta."}
p <- seq(0, 1, length = 100)

data.frame(
  p = p,
  p1 = dbeta(p, 1, 1),
  p2 = dbeta(p, 2, 2),
  p3 = dbeta(p, 4, 2),
  p4 = dbeta(p, 2, 4)
  ) %>%
  pivot_longer(p1:p4, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = p, y = prob / sum(prob), color = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.6) +
  # geom_line(color = "black", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_discrete(
    name = "Parameters",
    labels = c(
      "a = 1, b = 1",
      "a = 2, b = 2",
      "a = 4, b = 2",
      "a = 2, b = 4"
      )
    )
```

Le niveau de certitude augmente avec la somme $\kappa = a + b$

  - Aucune idée sur la provenance de la pièce : $a = b = 1$ -> **prior plat**
  - En attendant le début de l'expérience, on a lancé la pièce 10 fois et observé 5 "Face" : $a = b = 5$ -> **prior peu informatif**
  - La pièce provient de la banque de France :  $a = b = 50$ -> **prior fort**

```{r beta3, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Interprétation des paramètres d'une distribution Beta..."}
p <- seq(0, 1, length = 100)

data.frame(
  p = p,
  p1 = dbeta(p, 1, 1),
  p2 = dbeta(p, 5, 5),
  p3 = dbeta(p, 50, 50)
  ) %>%
  pivot_longer(p1:p3, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = p, y = prob / sum(prob), color = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.6) +
  # geom_line(color = "black", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_discrete(
    name = "Parameters",
    labels = c(
      "a = 1, b = 1",
      "a = 5, b = 5",
      "a = 50, b = 50"
      )
    )
```

Supposons que l'on dispose d'une estimation de la valeur la plus probable $\omega$ du paramètre $\theta$. On peut reparamétriser la distribution Beta en fonction du mode $\omega$ et du niveau de certitude $\kappa$ :

$$
\begin{aligned}
a &= \omega(\kappa - 2) + 1 \\
b &= (1 - \omega)(\kappa - 2) + 1 &&\mbox{pour } \kappa > 2
\end{aligned}
$$

Si $\omega = 0.65$ et $\kappa = 25$ alors $p(\theta) = \mathrm{Beta}(\theta \ | \ 15.95, 9.05)$. <br>
Si $\omega = 0.65$ et $\kappa = 10$ alors $p(\theta) = \mathrm{Beta}(\theta \ | \ 6.2, 3.8)$.

```{r beta4, echo = FALSE, fig.width = 12, fig.height = 5, fig.cap = "Blah blah..."}
p <- seq(0, 1, length = 100)

W <- 0.65
K1 <- 25
K2 <- 10

data.frame(
  p = p,
  p1 = dbeta(p, W * (K1 - 2) + 1, (1 - W) * (K1 - 2) + 1),
  p2 = dbeta(p, W * (K2 - 2) + 1, (1 - W) * (K2 - 2) + 1)
  ) %>%
  pivot_longer(p1:p2, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = p, y = prob / sum(prob), color = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.6) +
  # geom_line(color = "black", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_discrete(
    name = "Parameters",
    labels = c(
      expression(paste(omega, " = 0.65 ", kappa, " = 25", sep = "") ),
      expression(paste(omega, " = 0.65 ", kappa, " = 10", sep = "") )
      )
    )
```

### Prior conjugué

Formellement, si $\mathcal{F}$ est une classe de distributions d'échantillonnage $p(y|\theta)$, et $\mathcal{P}$ est une classe de distributions a priori pour $\theta$, alors $\mathcal{P}$ est **conjuguée** à $\mathcal{F}$ si et seulement si :

$$
p(\theta|y) \in \mathcal{P} \text{ for all } p(\cdot | \theta) \in \mathcal{F} \text{ and } p(\cdot) \in \mathcal{P}
$$

(Gelman et al., 2013, p.35). En d'autres termes, un prior est appelé **conjugué** si, lorsqu'il est converti en une distribution a posteriori en étant multiplié par la vraisemblance, il conserve la même forme. Dans notre cas, le prior Beta est un prior conjugué pour la vraisemblance binomiale, car le posterior est également une distribution Beta.

> Le résultat du produit d'un prior Beta et d'une fonction de vraisemblance  Binomiale est proportionnel à une distribution Beta. On dit alors que la distribution Beta est **un prior conjugué** de la fonction de vraisemblance Binomiale.

### Dérivation analytique de la distribution a posteriori

Soit un prior défini par : $\ \color{steelblue}{p(\theta \ | \ a, b) = \mathrm{Beta}(a, b) \propto \theta^{a - 1}(1 - \theta)^{b - 1}}$

Soit une fonction de vraisemblance associée à $y$ "Face" pour $n$ lancers : $\ \color{orangered}{p(y \ | \ n, \theta) = \mathrm{Bin}(y \ | \ n, \theta) = \left(\begin{array}{l} n \\ y \end{array}\right) \theta^{y}(1 - \theta)^{n - y} \propto \theta^{y}(1 - \theta)^{n - y}}$

$$
\begin{aligned}
\color{purple}{p(\theta \ | \ y, n)} &\propto \color{orangered}{p(y \ | \ n, \theta)} \ \color{steelblue}{p(\theta)} &&\mbox{Théorème de Bayes} \\
&\propto \color{orangered}{\mathrm{Bin}(y \ | \ n, \theta)} \ \color{steelblue}{\mathrm{Beta}(\theta \ | \ a, b)} \\
&\propto \color{orangered}{\theta^{y}(1 - \theta)^{n - y}} \ \color{steelblue}{\theta^{a - 1}(1 - \theta)^{b - 1}} &&\mbox{Application des formules précédentes} \\
&\propto \color{purple}{\theta}^{\color{orangered}{y} + \color{steelblue}{a - 1}}\color{purple}{(1 - \theta)}^{\color{orangered}{n - y} + \color{steelblue}{b - 1}} &&\mbox{En regroupant les termes identiques} \\
&\propto \color{purple}{\theta^{a' - 1}(1 - \theta)^{b' - 1}} &&\mbox{Avec } a' = y + a \mbox{ et } b' = n - y + b \\
\color{purple}{p(\theta \ | \ y, n)} \ &= \color{purple}{\mathrm{Beta}(y + a, n - y + b)}
\end{aligned}
$$

### Un exemple pour digérer

On observe $y = 7$ réponses correctes sur $n = 10$ questions. On choisit un prior $\mathrm{Beta}(1, 1)$, c'est à dire un prior uniforme sur $[0, 1]$. Ce prior équivaut à une connaissance a priori de 0 succès et 0 échecs (i.e., prior plat).

La distribution postérieure est donnée par :

$$
\begin{aligned}
\color{purple}{p(\theta \ | \ y, n)} &\propto \color{orangered}{p(y \ | \ n, \theta)} \ \color{steelblue}{p(\theta)} \\
&\propto \color{orangered}{\mathrm{Bin}(7 \ | \ 10, \theta)} \ \color{steelblue}{\mathrm{Beta}(\theta \ | \ 1, 1)} \\
&= \color{purple}{\mathrm{Beta}(y + a, n - y + b)} \\
&= \color{purple}{\mathrm{Beta}(8, 4)}
\end{aligned}
$$

La moyenne de la distribution postérieure est donnée par :

$$
\color{purple}{\underbrace{\frac{y + a}{n + a + b}}_{posterior}} = \color{orangered}{\underbrace{\frac{y}{n}}_{data}} \underbrace{\frac{n}{n + a + b}}_{weight} + \color{steelblue}{\underbrace{\frac{a}{a + b}}_{prior}} \underbrace{\frac{a + b}{n + a + b}}_{weight}
$$

Blah blah...

```{r beta-exemple, echo = FALSE, fig.width = 9, fig.height = 9, fig.cap = "Exemple..."}
library(patchwork)

df <- data.frame(theta = seq(0, 1, length = 100) ) %>%
  mutate(
    prior = dbeta(p, 1, 1),
    likelihood = dbinom(7, 10, p)
    ) %>%
  mutate(posterior = (prior * likelihood) )
  # pivot_longer(prior:posterior, names_to = "params", values_to = "prob")

p1 <- df %>%
  ggplot(aes(x = theta, y = prior, colour = NULL) ) +
  geom_area(
    fill = "steelblue",
    position = "identity", alpha = 0.5
    ) +
 # geom_line(color = "steelblue", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  ylim(0, 2) +
  ggtitle("Prior distribution Beta(1, 1)")

p2 <- df %>%
  ggplot(aes(x = theta, y = likelihood, colour = NULL) ) +
  geom_area(
    fill = "orangered",
    position = "identity", alpha = 0.5
    ) +
  # geom_line(color = "steelblue", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  # ylim(0, 2) +
  ggtitle("Likelihood function Bin(7, 10)")

p3 <- df %>%
  ggplot(aes(x = theta, y = posterior, colour = NULL) ) +
  geom_area(
    fill = "purple",
    position = "identity", alpha = 0.5
    ) +
  # geom_line(color = "steelblue", size = 0.2) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  # ylim(0, 2) +
  ggtitle("Posterior distribution Beta(8, 4)")

p1 / p2 / p3
```

### Influence du prior sur la distribution postérieure

Cas $n < a + b, (n = 10, a = 4, b = 16)$.

```{r posterior-exemple1, echo = FALSE, fig.width = 12, fig.height = 8, fig.cap = "Influence du prior..."}
a <- 4
b <- 16
y <- 6
n <- 10

data.frame(theta = seq(0, 1, length = 100) ) %>%
  mutate(
    prior = dbeta(p, a, b),
    likelihood = dbinom(y, n, p)
    ) %>%
  mutate(prior = prior / sum(prior) ) %>%
  mutate(likelihood = likelihood / sum(likelihood) ) %>%
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood) ) %>%
  pivot_longer(prior:posterior, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = theta, y = prob, colour = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_manual(
    name = "",
    labels = c("Likelihood", "Posterior", "Prior"),
    values = c("orangered", "purple", "steelblue")
    )
```

Cas $n = a + b, (n = 20, a = 4, b = 16)$.

```{r posterior-exemple2, echo = FALSE, fig.width = 12, fig.height = 8, fig.cap = "Influence du prior..."}
a <- 4
b <- 16
y <- 12
n <- 20

data.frame(theta = seq(0, 1, length = 100) ) %>%
  mutate(
    prior = dbeta(p, a, b),
    likelihood = dbinom(y, n, p)
    ) %>%
  mutate(prior = prior / sum(prior) ) %>%
  mutate(likelihood = likelihood / sum(likelihood) ) %>%
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood) ) %>%
  pivot_longer(prior:posterior, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = theta, y = prob, colour = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_manual(
    name = "",
    labels = c("Likelihood", "Posterior", "Prior"),
    values = c("orangered", "purple", "steelblue")
    )
```

Cas $n > a + b, (n = 40, a = 4, b = 16)$.

```{r posterior-exemple3, echo = FALSE, fig.width = 12, fig.height = 8, fig.cap = "Influence du prior..."}
a <- 4
b <- 16
y <- 24
n <- 40

data.frame(theta = seq(0, 1, length = 100) ) %>%
  mutate(
    prior = dbeta(p, a, b),
    likelihood = dbinom(y, n, p)
    ) %>%
  mutate(prior = prior / sum(prior) ) %>%
  mutate(likelihood = likelihood / sum(likelihood) ) %>%
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood) ) %>%
  pivot_longer(prior:posterior, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = theta, y = prob, colour = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_manual(
    name = "",
    labels = c("Likelihood", "Posterior", "Prior"),
    values = c("orangered", "purple", "steelblue")
    )
```

### Ce qu'il faut retenir

> The posterior distribution is always a compromise between the prior distribution and the likelihood function. <br>
> *Kruschke (2015)*

```{r posterior-exemple4, echo = FALSE, fig.width = 14, fig.height = 7, fig.cap = "Influence du prior..."}
a <- 3
b <- 17
y <- 8
n <- 10

data.frame(theta = seq(0, 1, length = 100) ) %>%
  mutate(
    prior = dbeta(p, a, b),
    likelihood = dbinom(y, n, p)
    ) %>%
  mutate(prior = prior / sum(prior) ) %>%
  mutate(likelihood = likelihood / sum(likelihood) ) %>%
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood) ) %>%
  pivot_longer(prior:posterior, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = theta, y = prob, colour = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_manual(
    name = "",
    labels = c("Likelihood", "Posterior", "Prior"),
    values = c("orangered", "purple", "steelblue")
    )
```

Plus on a de données, moins le prior a d'influence dans l'estimation de la distribution a posteriori (et réciproquement).

Attention : Lorsque le prior accorde une probabilité de 0 à certaines valeurs de $\theta$, le modèle est incapable d'apprendre (ces valeurs sont alors considérées comme "impossibles")...

```{r posterior-exemple5, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Influence du prior..."}
y <- 8
n <- 10

data.frame(theta = seq(0, 1, length = 100) ) %>%
  mutate(
    prior = dunif(p, 0.25, 0.75),
    likelihood = dbinom(y, n, p)
    ) %>%
  mutate(prior = prior / sum(prior) ) %>%
  mutate(likelihood = likelihood / sum(likelihood) ) %>%
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood) ) %>%
  pivot_longer(prior:posterior, names_to = "params", values_to = "prob") %>%
  ggplot(aes(x = theta, y = prob, colour = NULL, fill = params) ) +
  geom_area(position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density") +
  scale_fill_manual(
    name = "",
    labels = c("Likelihood", "Posterior", "Prior"),
    values = c("orangered", "purple", "steelblue")
    )
```

### La vraisemblance marginale (the devil is in the denominator)

$$
Posterior = \frac{Likelihood \ \times \ Prior}{Marginal \ Likelihood} \propto Likelihood \ \times \ Prior
$$

$$
p(\theta \ | \ data) = \frac{p(data \ | \ \theta) \ \times \ p(\theta)}{p(data)} \propto p(data \ | \ \theta) \ \times \ p(\theta)
$$ 

Si on zoom sur la vraisemblance marginale (aussi connue comme *evidence*)...

$$
\begin{aligned}
\color{green}{p(data)} &= \int p(data,\theta) \, \mathrm d\theta &&\text{Marginalisation sur le paramètre } \theta \\
\color{green}{p(data)} &= \color{green}{\int p(data \ | \ \theta) \ p(\theta) \, \mathrm{d} \theta} && \text{Application de la règle du produit}
\end{aligned}
$$

Petit problème : $p(data)$ se calcule en calculant la somme (pour des variables discrètes) ou l'intégrale (pour des variables continues) de la densité conjointe $p(data, \theta)$ sur toutes les valeurs possibles de $\theta$. Cela se complique lorsque le modèle comprend plusieurs paramètres.

Par exemple pour deux paramètres discrets :

$$
p(data) = \sum_{\theta_{1}} \sum_{\theta_{2}} p(data, \theta_{1}, \theta_{2})
$$

Et pour un modèle avec deux paramètres continus :

$$
p(data) = \int\limits_{\theta_{1}} \int\limits_{\theta_{2}} p(data, \theta_{1}, \theta_{2}) \mathrm{d} \theta_{1} \mathrm{d} \theta_{2}
$$

Trois méthodes pour résoudre (contourner) ce problème :

1. Solution analytique $~\longrightarrow~$ Utilisation d'un prior conjugué (e.g., le modèle  Beta-Binomial)

2. Solution discrètisée $~\longrightarrow~$ Calcul de la solution sur un ensemble fini de points (grid method)

3. Solution approchée $~\longrightarrow~$ On échantillonne "intelligemment" l'espace conjoint des paramètres (méthodes MCMC, cf. cours n°05)

### La distribution postérieure, solution analytique

#### Distributions discrètes

```{r discrete, echo = FALSE, out.width = "100%", fig.cap = "Illustration of the Mersenne-Twister algorithm... Figure from..."}
knitr::include_graphics("figures/discrete.png")
```

#### Distributions continues

```{r continuous, echo = FALSE, out.width = "100%", fig.cap = "Illustration of the Mersenne-Twister algorithm... Figure from..."}
knitr::include_graphics("figures/continuous.png")
```

Problème : Cette solution est très contraignante. Idéalement, le modèle (likelihood + prior) devrait être défini à partir de l'interprétation que l'on peut faire des paramètres de ces distributions, et non pour faciliter les calculs...

### La distribution postérieure, grid method

1. **Définir la grille**
2. Calculer la valeur du prior pour chaque valeur de la grille
3. Calculer la valeur de la vraisemblance pour chaque valeur de la grille
4. Calculer le produit prior x vraisemblance pour chaque valeur de la grille, puis normalisation du résultat

```{r grid1, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
thetaSize <- 30

data.frame(
  theta = seq(from = 0, to = 1, length.out = thetaSize),
  pTheta = rep(.03, thetaSize)
  ) %>% 
  ggplot(aes(x = theta, y = 0, xend = theta) ) +
  geom_segment(aes(yend = pTheta), size = 1, colour = "#339900") +
  ylim(0., 0.15) +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20)
```

1. Définir la grille
2. **Calculer la valeur du prior pour chaque valeur de la grille**
3. Calculer la valeur de la vraisemblance pour chaque valeur de la grille
4. Calculer le produit prior x vraisemblance pour chaque valeur de la grille, puis normalisation du résultat

```{r grid2, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
thetaSize <- 30
a <- 3
b <- 7

data.frame(
  theta = seq(from = 0, to = 1, length.out = thetaSize)
  ) %>%
  mutate(
    prior = dbeta(theta, 3, 7) / sum(dbeta(theta, 3, 7) )
  ) %>% 
  ggplot(aes(x = theta, y = 0, xend = theta) ) +
  geom_segment(aes(yend = prior), size = 1, colour = "steelblue") +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20)
```

1. Définir la grille
2. Calculer la valeur du prior pour chaque valeur de la grille
3. **Calculer la valeur de la vraisemblance pour chaque valeur de la grille**
4. Calculer le produit prior x vraisemblance pour chaque valeur de la grille, puis normalisation du résultat

```{r grid3, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
thetaSize <- 30
a <- 3
b <- 7
y <- 12
n <- 20

data.frame(
  theta = seq(from = 0, to = 1, length.out = thetaSize)
  ) %>%
  mutate(
    prior = dbeta(theta, 3, 7) / sum(dbeta(theta, 3, 7) )
  ) %>% 
  mutate(
    likelihood = dbinom(y, n, theta) / sum(dbinom(y, n, theta) )
  ) %>% 
  ggplot(aes(x = theta, y = 0, xend = theta) ) +
  geom_segment(aes(yend = prior), size = 1, colour = "steelblue") +
  geom_segment(aes(
    x = theta + 0.01, yend = likelihood, xend = theta + 0.01),
    colour = "orangered", size = 1
    ) +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20)
```

1. Définir la grille
2. Calculer la valeur du prior pour chaque valeur de la grille
3. Calculer la valeur de la vraisemblance pour chaque valeur de la grille
4. **Calculer le produit prior x vraisemblance pour chaque valeur de la grille, puis normalisation du résultat**

```{r grid4, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
thetaSize <- 30
a <- 3
b <- 7
y <- 12
n <- 20

data.frame(
  theta = seq(from = 0, to = 1, length.out = thetaSize)
  ) %>%
  mutate(
    prior = dbeta(theta, 3, 7) / sum(dbeta(theta, 3, 7) )
  ) %>% 
  mutate(
    likelihood = dbinom(y, n, theta) / sum(dbinom(y, n, theta) )
  ) %>%
  mutate(
    posterior = (prior * likelihood) / sum(prior * likelihood)
  ) %>% 
  ggplot(aes(x = theta, y = 0, xend = theta) ) +
  geom_segment(aes(yend = prior), size = 1, colour = "steelblue") +
  geom_segment(aes(
    x = theta + 0.01, yend = likelihood, xend = theta + 0.01),
    colour = "orangered", size = 1
    ) +
  geom_segment(aes(
    x = theta - 0.01, yend = posterior, xend = theta - 0.01),
    colour = "purple", size = 1
    ) +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20)
```

1. Définir la grille
2. Calculer la valeur du prior pour chaque valeur de la grille
3. Calculer la valeur de la vraisemblance pour chaque valeur de la grille
4. **Calculer le produit prior x vraisemblance pour chaque valeur de la grille, puis normalisation du résultat**

```{r grid5, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
thetaSize <- 100
a <- 3
b <- 7
y <- 12
n <- 20

data.frame(
  theta = seq(from = 0, to = 1, length.out = thetaSize)
  ) %>%
  mutate(
    prior = dbeta(theta, 3, 7) / sum(dbeta(theta, 3, 7) )
  ) %>% 
  mutate(
    likelihood = dbinom(y, n, theta) / sum(dbinom(y, n, theta) )
  ) %>%
  mutate(
    posterior = (prior * likelihood) / sum(prior * likelihood)
  ) %>% 
  ggplot(aes(x = theta, y = 0, xend = theta) ) +
  geom_segment(aes(yend = prior), size = 1, colour = "steelblue") +
  geom_segment(aes(
    x = theta + 0.01, yend = likelihood, xend = theta + 0.01),
    colour = "orangered", size = 1
    ) +
  geom_segment(aes(
    x = theta - 0.01, yend = posterior, xend = theta - 0.01),
    colour = "purple", size = 1
    ) +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20)
```

Problème du nombre de paramètres... En affinant la grille on augmente le temps de calcul :

- 3 paramètres avec une grille de $10^3$ noeuds = une grille de $10^9$ points de calcul
- 10 paramètres avec une grille de $10^3$ noeuds = une grille de $10^{30}$ points de calcul
 
Le "superordinateur" chinois Tianhe-2 réalise $33,8 \text{×} 10^{15}$ opérations par seconde. Si on considère qu'il réalise 3 opérations par noeud de la grille, il lui faudrait $10^{14}$ secondes pour parcourir la grille une fois (pour comparaison, l'âge de l'univers est approximativement de $(4,354 ± 0,012)\text{×}10^{17}$ secondes)...

### Échantillonner la distribution postérieure

Pour échantillonner une distribution postérieure, on peut utiliser une approximation par grille ou différentes implémentations des méthodes MCMC (e.g., Metropolis-Hasting, Gibbs, Hamilton, cf. Cours n°05).

En pratique :

```{r sampling1, fig.width = 8, fig.height = 4, fig.cap = "Blah blah..."}
p_grid <- seq(from = 0, to = 1, length.out = 1000) # creating a grid
prior <- rep(1, 1000) # uniform prior
likelihood <- dbinom(y, size = n, prob = p_grid) # computes likelihood
posterior <- (likelihood * prior) / sum(likelihood * prior) # computes posterior
samples <- sample(posterior, size = 1e3, prob = posterior, replace = TRUE) # sampling
hist(samples, main = "", xlab = expression(theta), cex.axis = 1, cex.lab = 1.5) # histogram
```

La précision dépend de la taille de l'échantillon...

```{r sampling2, eval = TRUE, echo = FALSE, fig.width = 20, fig.height = 5, fig.cap = "Blah blah..."}
set.seed(666)

trajLength <- 100
theta <- 1:7
ptheta <- theta

trajectory <- sample(
  theta, prob = ptheta, size = trajLength, replace = TRUE
  )

layout(matrix(1:2, ncol = 2), widths = c(0.75, 0.25) )

plot(
  trajectory,
  main = "Distribution postérieure basée sur 100 tirages",
  ylab = expression(theta),
  xlim = c(0, trajLength),
  xlab = "Nombre d'itérations",
  type = "o", pch = 20, col = "purple",
  cex.lab = 2, cex.main = 2, cex.axis = 1.5
  )

barplot(
  table(trajectory), col = "purple",
  horiz = TRUE, axes = FALSE, axisnames = FALSE
  )
```

```{r sampling3, eval = TRUE, echo = FALSE, fig.width = 20, fig.height = 5, , fig.cap = "Blah blah..."}
set.seed(666)

trajLength <- 1000
theta <- 1:7
ptheta <- theta

trajectory <- sample(
  theta, prob = ptheta, size = trajLength, replace = TRUE
  )

layout(matrix(1:2, ncol = 2), widths = c(0.75, 0.25) )

plot(
  trajectory,
  main = "Distribution postérieure basée sur 1000 tirages",
  ylab = expression(theta),
  xlim = c(0, trajLength),
  xlab = "Nombre d'itérations",
  type = "o", pch = 20, col = "purple",
  cex.lab = 2, cex.main = 2, cex.axis = 2
  )

barplot(
  table(trajectory), col = "purple",
  horiz = TRUE, axes = FALSE, axisnames = FALSE
  )
```

### La distribution postérieure, résumé

- Cas analytique :

```{r, eval = FALSE, echo = TRUE}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
a <- b <- 1 # parameters of the Beta prior
n <- 9 # number of observations
y <- 6 # number of successes
posterior <- dbeta(p_grid, z + a - 1, N - z + b - 1)
```

- Grid method :

```{r, eval = FALSE, echo = TRUE}
p_grid <- seq( from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000) # uniform prior
likelihood <- dbinom(y, size = n, prob = p_grid)
posterior <- (likelihood * prior) / sum(likelihood * prior)
```

- Échantillonner la distribution postérieure :

```{r, eval = FALSE, echo = TRUE}
sample(data, size = trajLength, prob = prob, replace = TRUE)
```

**Méthode analytique**
  * La distribution postérieure est décrite explicitement
  * Le modèle est fortement contraint 

**Méthode Grid**
 * La distribution postérieure n'est donnée que pour un ensemble fini de valeurs 
 * Plus la grille est fine, meilleure est l'estimation de la distribution postérieure 
 * Compromis *Précision - Temps de calcul*

### Utiliser les échantillons pour résumer la distribution postérieure

#### Estimation de la tendance centrale

À partir d'un ensemble d'échantillons d'une distribution postérieure, on peut calculer la moyenne, le mode, et la médiane. Par exemple pour un prior uniforme, 10 lancers et 3 Faces.

```{r tendance-centrale1, eval = FALSE, echo = TRUE}
mode_posterior <- find_mode(samples) # in blue
mean_posterior <- mean(samples) # in orange
median_posterior <- median(samples) # in green
```

```{r tendance-centrale2, echo = FALSE, fig.width = 18, fig.height = 6, fig.cap = "Blah blah..."}
set.seed(666)
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(x = 3, size = 10, prob = p_grid)
posterior <- (likelihood * prior) / sum(likelihood * prior)
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

find_mode <- function(samples, ...) {
  
    dd <- density(samples, ...)
    dd$x[which.max(dd$y)]
    
}

mode_posterior <- find_mode(samples)
mean_posterior <- mean(samples)
median_posterior <- median(samples)

data.frame(theta = p_grid, posterior = posterior) %>%
  ggplot(aes(x = theta, y = posterior, colour = NULL) ) +
  geom_area(position = "identity", fill = "purple", alpha = 0.25) +
  geom_vline(aes(xintercept = mode_posterior), size = 1, color = "steelblue") +
  annotate(
    geom = "text",
    x = mode_posterior - 0.01, y = max(posterior) / 2,
    label = "mode", color = "steelblue", angle = 90, size = 5
    ) +
  geom_vline(aes(xintercept = mean_posterior), size = 1, color = "orangered") +
  annotate(
    geom = "text",
    x = mean_posterior + 0.01, y = max(posterior) / 2,
    label = "mean", color = "orangered", angle = 90, size = 5
    ) +
  geom_vline(aes(xintercept = median_posterior), size = 1, color = "forestgreen") +
  annotate(
    geom = "text",
    x = median_posterior - 0.01, y = max(posterior) / 2,
    label = "median", color = "forestgreen", angle = 90, size = 5
    ) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density")
```

Quelle est la probabilité que le biais de la pièce $\theta$ soit supérieur à 0.5 ?

```{r superiority-prob, eval = TRUE, echo = TRUE}
sum(samples > 0.5) / length(samples) # length(samples) is the number of samples
```

Quelle est la probabilité que le biais de la pièce $\theta$ soit compris entre 0.2 et 0.4 ?

```{r interval-prob, eval = TRUE, echo = TRUE}
sum(samples > 0.2 & samples < 0.4) / 1e4 # length(samples) is the number of samples
```

```{r interval-prob-plot, echo = FALSE, fig.width = 12, fig.height = 4, fig.cap = "Blah blah..."}
df <- data.frame(theta = p_grid, posterior = posterior)

ggplot(df, aes(x = theta, y = posterior, colour = NULL) ) +
    geom_area(
    position = "identity", fill = "purple", alpha = 0.5
    ) +
  geom_area(
    data = subset(df, theta > 0.4),
    position = "identity", fill = "purple", alpha = 0.5
    ) +
  geom_area(
    data = subset(df, 0.2 > theta),
    position = "identity", fill = "purple", alpha = 0.5
    ) +
  theme_bw(base_size = 20) +
  xlab(expression(theta) ) +
  ylab("Probability density")
```

### Highest density interval (HDI)

- Le HDI indique les valeurs du paramètre qui sont les plus probables (sachant les données et le prior)
- Plus le HDI est étroit et plus le degré de certitude est élevé
- La largeur du HDI diminue avec l'augmentation du nombre de mesures

> Définition: les valeurs du paramètre $\theta$ contenues dans un HDI à 89% sont telles que $p(\theta) > W$ où $W$ satisfait la condition suivante :
>
> $$\int_{\theta \ : \ p(\theta) > W} p(\theta) \, \mathrm{d} \theta = 0.89.$$

```{r hdi, echo = FALSE, out.width = "100%", fig.cap = "Illustration of the Mersenne-Twister algorithm... Figure from..."}
knitr::include_graphics("figures/HDI.png")
```

```{r plotpost1, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
library(BEST)

set.seed(666)
p_grid <- seq(from = 0, to = 1, length.out = 1e3)
pTheta <- dbeta(p_grid, 3, 10)
massVec <- pTheta / sum(pTheta)
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = pTheta)

plotPost(samples, credMass = 0.89, cex = 1.5, xlab = expression(theta), xlim = c(0, 1) )
```

### Region of practical equivalence (ROPE)

On l'utilise pour tester une hypothèse :

- La valeur du paramètre (e.g., $\theta = 0.5$) est rejetée si le HDI est entièrement hors de la ROPE
- La valeur du paramètre (e.g., $\theta = 0.5$)  est acceptée si le HDI est entièrement dans la ROPE
- Si le HDI et la ROPE se chevauchent on ne peut pas conclure...

```{r rope, eval = TRUE, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "Blah blah..."}
library(BEST)

set.seed(666)
p_grid <- seq(from = 0, to = 1, length.out = 1e3)
pTheta <- dbeta(p_grid, 3, 10)
massVec <- pTheta / sum(pTheta)
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = pTheta)

plotPost(
  samples, credMass = 0.89,
  cex = 2, cex.axis = 1.5, cex.lab = 2,
  xlab = expression(theta), xlim = c(0, 1),
  ROPE = c(0.49, 0.51)
  )
```

### Model checking

Les deux rôles de la fonction de vraisemblance :

+ C'est une fonction de $\theta$ pour le calcul de la distribution postérieure : $\mathcal{L}(\theta \ | \ y, n)$
+ Lorsque $\theta$ est connu / fixé, c'est une distribution de probabilité : $p(y \ |\ \theta, n) = \theta^y(1 - \theta)^{(n - y)}$

On peut utiliser cette distribution de probabilité pour générer des données... !

Par exemple : Générer 10000 valeurs à partir d'une loi binomiale basée sur 9 lancers et une probabilité de Face de 0.6 :

```{r eval = TRUE, echo = TRUE}
samples <- rbinom(n = 1e4, size = 10, prob = 0.6)
```

Deux sources d'incertitude dans ces prédictions :

+ Incertitude liée au processus d'échantillonnage <br>
    -> Chaque valeur apparaît avec une probabilité $\theta$
+ Incertitude sur la valeur de $\theta$ elle-même <br>
    -> Pour chaque valeur de $\theta$ on peut calculer une distribution implicite 

Par exemple : Générer 10000 valeurs à partir d'une loi binomiale basé sur 9 lancers et une probabilité de Face décrite par la distribution postérieure de $\theta$ :

```{r eval = TRUE, echo = TRUE}
samples <- rbinom(n = 1e4, size = 10, prob = rbeta(1e4, 16, 10) )
```

### Posterior predictive checking

```{r ppc, echo = FALSE, fig.width = 12, fig.height = 8, fig.cap = "Blah blah..."}
thetaSize <- 1e3
a <- 3
b <- 7
y <- 9
n <- 10

df <- data.frame(
  theta = seq(from = 0, to = 1, length.out = thetaSize)
  ) %>%
  mutate(
    prior = dbeta(theta, a, b) / sum(dbeta(theta, a, b) ),
    likelihood = dbinom(y, n, theta) / sum(dbinom(y, n, theta) )
    ) %>%
  mutate(
    unnormalised_posterior = prior * likelihood,
    posterior = unnormalised_posterior / sum(unnormalised_posterior)
    ) %>%
  mutate(
    # prior predictive distribution
    ppc1 = rbinom(thetaSize, n, rbeta(thetaSize, a, b) ),
    # posterior predictive distribution
    ppc2 = rbinom(thetaSize, n, rbeta(thetaSize, y + a, n - y + b) )
    )
  
p1 <- df %>%
  ggplot(aes(x = theta, y = prior) ) +
  geom_area(aes(colour = NULL), stat = "identity", fill = "steelblue", alpha = 0.8) +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20) +
  ggtitle("Prior distribution")

p2 <- df %>%
  ggplot(aes(x = ppc1) ) +
  geom_histogram(aes(colour = NULL), fill = "black") +
  theme_bw(base_size = 20) +
  ggtitle("Prior predictive distribution") +
  labs(x = "Number of heads", y = "Count") +
  scale_x_continuous(breaks = seq(0, 10, 1), labels = seq(0, 10, 1) )

p3 <- df %>%
  ggplot(aes(x = theta, y = posterior) ) +
  geom_area(aes(colour = NULL), stat = "identity", fill = "purple", alpha = 0.8) +
  labs(x = expression(theta), y = "Probability density") +
  theme_bw(base_size = 20) + ggtitle("Posterior distribution")

p4 <- df %>%
  ggplot(aes(x = ppc2) ) +
  geom_histogram(aes(colour = NULL), fill = "black") +
  theme_bw(base_size = 20) +
  ggtitle("Posterior predictive distribution") +
  labs(x = "Number of heads", y = "Count") +
  scale_x_continuous(breaks = seq(0, 10, 1), labels = seq(0, 10, 1) )

library(patchwork)

(p1 | p2) / (p3 | p4) # stacking plots vertically
```

Blah blah...

```{r ppc-rethinking, echo = FALSE, out.width = "100%", fig.cap = "Illustration of the posterior predictive checking procedure. Figure from McElreath (2016)."}
knitr::include_graphics("figures/ModelPredictions.jpg")
```

## Conclusions

...
