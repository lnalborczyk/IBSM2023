# Modèle de régression linéaire {#linear-regression1}

```{r setup-ch3, include = FALSE, message = FALSE}
library(tidyverse)
library(knitr)

# setting up knitr options
opts_chunk$set(
  cache = TRUE, echo = TRUE, warning = FALSE, message = FALSE,
  fig.align = "center", out.width = "75%", fig.pos = "!htb"
  )
```

\definecolor{steelblue}{RGB}{70, 130, 180}
\definecolor{green}{RGB}{0, 153, 0}
\definecolor{purple}{RGB}{153, 0, 153}
\definecolor{orangered}{cmyk}{0, 73, 100, 0}

Introduction au chapitre blah blah...

## Langage de la modélisation

$$
\begin{aligned}
y_{i} &\sim \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i}&= \alpha + \beta x_{i} \\
\alpha &\sim \mathrm{Normal}(60, 10) \\
\beta &\sim \mathrm{Normal}(0, 10) \\
\sigma &\sim \mathrm{HalfCauchy}(0, 1)
\end{aligned}
$$

**Objectif de la séance** : comprendre ce type de modèle.

Les constituants de nos modèles seront toujours les mêmes et nous suivrons les deux mêmes étapes : 

- Construire le modèle (*likelihood* + *priors*).
- Mettre à jour grâce aux données (*updating*), afin de calculer la distribution postérieure.

## Un premier modèle

```{r importing-howell, eval = TRUE, echo = TRUE}
library(rethinking)
library(tidyverse)

data(Howell1)
d <- Howell1
str(d)
```

```{r filtering-howell, eval = TRUE, echo = TRUE}
d2 <- d %>% filter(age >= 18)
head(d2)
```

...

$$h_{i} \sim \mathrm{Normal}(\mu, \sigma)$$

```{r echo = TRUE, fig.width = 5, fig.height = 5}
d2 %>%
    ggplot(aes(x = height) ) +
    geom_histogram(bins = 10, col = "white") +
    theme_bw(base_size = 18)
```

## Loi normale

$$
p(x \ | \ \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg[-\frac{1}{2 \sigma^{2}} (\mu - x)^{2} \bigg]
$$

```{r eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 6}
data.frame(value = rnorm(1e4, 10, 1) ) %>% # 10000 samples from Normal(10, 1)
    ggplot(aes(x = value) ) +
    geom_histogram(col = "white") +
    theme_bw(base_size = 20)
```

### D'où vient la loi normale ?

Certaines valeurs sont fortement probables (autour de la moyenne $\mu$). Plus on s'éloigne, moins les valeurs sont probables (en suivant une décroissance exponentielle).

```{r normal-explain1, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
f1 <- function(x) {exp(-x)}
f2 <- function(x) {exp(-x^2)}

ggplot(data.frame(x = c(0, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f1, lwd = 1) +
  geom_path(aes(colour = "orangered"), stat = "function", fun = f2, lwd = 1) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-x)", "y = exp(-x^2)"),
    breaks = c("steelblue", "orangered")
    ) +
  theme_bw(base_size = 20)
```

$$
y = \exp \big[-x^{2} \big]
$$

On étend notre fonction aux valeurs négatives.

```{r normal-explain2, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f2, lwd = 1) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-x^2)", "y = exp(-x^2)"),
    breaks = c("steelblue", "orangered")
    ) +
  theme_bw(base_size = 20)
```

$$
y = \exp \big[-x^{2} \big]
$$

Les points d'inflection nous donnent une bonne indication de là où la plupart des valeurs se trouvent (i.e., entre les points d'inflection). Les pics de la dérivée nous montrent les points d'inflection.

```{r normal-explain3, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
f <- expression(exp(-x^2) )
f_derivative <- D(f, "x")

f3 <- function(x) {-(exp(-x^2) * (2 * x) )}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f2, lwd = 1) +
  geom_path(aes(colour = "orangered"), stat = "function", fun = f3, lwd = 1) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-x^2)", "derivative"),
    breaks = c("steelblue", "orangered")
    ) +
  theme_bw(base_size = 20)
```

$$
y = \exp \bigg [- \frac{1}{2} x^{2} \bigg]
$$

Ensuite on standardise la distribution de manière à ce que les deux points d'inflection se trouvent à $x = -1$ et $x = 1$.

```{r normal-explain4, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
f <- expression(exp((-0.5) * x^2) )
f_derivative <- D(f, "x")

f4 <- function(x) {exp((-0.5) * x^2)}
f5 <- function(x) {exp((-0.5) * x^2) * ((-0.5) * (2 * x) )}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f4, lwd = 1) +
  geom_path(aes(colour = "orangered"), stat = "function", fun = f5, lwd = 1) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-0.5x^2)", "derivative"),
    breaks = c("steelblue", "orangered")
    ) +
  theme_bw(base_size = 20)
```


$$
y = \exp \bigg [- \frac{1}{2 \color{steelblue}{\sigma^{2}}} x^{2} \bigg]
$$

On insère un paramètre $\sigma^{2}$ pour contrôler la distance entre les points d'inflection.

```{r normal-explain5, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
f4 <- function(x) {exp((-0.5) * x^2)}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(
    aes(colour = "steelblue"),
    colour = "steelblue",
    stat = "function", fun = f4, lwd = 1
    ) +
  labs(x = "x", y = "f(x)") +
  theme_bw(base_size = 20)
```

$$
y = \exp \bigg [- \frac{1}{2 \color{steelblue}{\sigma^{2}}} (x - \color{orangered}{\mu})^{2} \bigg]
$$

On insère ensuite un paramètre $\mu$ afin de pouvoir contrôler la position (la tendance centrale) de la distribution.

```{r normal-explain6, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
f4 <- function(x) {exp((-0.5) * (x - 3)^2)}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(
    aes(colour = "steelblue"),
    colour = "steelblue",
    stat = "function", fun = f4, lwd = 1
    ) +
  labs(x = "x", y = "f(x)") +
  theme_bw(base_size = 20)
```

$$
y = \frac{1}{\sqrt{2 \pi \color{steelblue}{\sigma^{2}}}} \exp \bigg[-\frac{1}{2 \color{steelblue}{\sigma^{2}}} (\color{orangered}{\mu} - x)^{2} \bigg]
$$

Mais... cette distribution n'intègre pas à 1. On divise donc par une constante de normalisation (la partie gauche), afin d'obtenir une distribution de probabilité.

```{r normal-explain7, echo = FALSE, fig.width = 12, fig.height = 6, fig.cap = "blah blah..."}
ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(
    aes(colour = "steelblue"),
    colour = "steelblue",
    stat = "function", fun = dnorm, lwd = 1
    ) +
  labs(x = "x", y = "f(x)") +
  theme_bw(base_size = 20)
```

## Modèle gaussien

Nous allons construire un modèle de régression, mais avant d'ajouter un prédicteur, essayons de modéliser la distribution des tailles.

On cherche à savoir quel est le modèle (la distribution) qui décrit le mieux la répartition des tailles. On va donc explorer toutes les combinaisons possibles de $\mu$ et $\sigma$ et les classer par leurs probabilités respectives. 

Notre but, une fois encore, est de décrire **la distribution postérieure**, qui sera donc d'une certaine manière **une distribution de distributions**.

On définit ensuite $p(\mu,\sigma)$, la distribution a priori conjointe de tous les paramètres du modèle. On peut spécifier ces priors indépendamment pour chaque paramètre, sachant que $p(\mu, \sigma) = p(\mu) p(\sigma)$.

$$\color{steelblue}{\mu \sim \mathrm{Normal}(178,20)}$$

```{r eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 9, fig.height = 6, fig.cap = "blah blah..."}
data.frame(x = c(100, 250) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dnorm, args = list(mean = 178, sd = 20),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  theme_bw(base_size = 20) +
  labs(x = expression(mu), y = "Probability density")
```

On définit ensuite $p(\mu,\sigma)$, la distribution a priori conjointe de tous les paramètres du modèle. On peut spécifier ces priors indépendamment pour chaque paramètre, sachant que $p(\mu, \sigma) = p(\mu) p(\sigma)$.

$$\color{steelblue}{\sigma \sim \mathrm{Uniform}(0,50)}$$

```{r plot-prior1, eval = TRUE, echo = FALSE, fig.width = 9, fig.height = 6, fig.cap = "blah blah..."}
data.frame(x = c(-10, 60) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dunif, args = list(0, 50),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  theme_bw(base_size = 20) +
  labs(x = expression(sigma), y = "Probability density")
```

## Visualiser le prior

```{r plot-prior-2D, eval = FALSE, echo = TRUE, fig.cap = "blah blah..."}
library(ks)
sample_mu <- rnorm(1e4, 178, 20) # prior on mu
sample_sigma <- runif(1e4, 0, 50) # prior on sigma
prior <- data.frame(cbind(sample_mu, sample_sigma) ) # multivariate prior
H.scv <- Hscv(x = prior, verbose = TRUE)
fhat_prior <- kde(x = prior, H = H.scv, compute.cont = TRUE)
plot(
    fhat_prior, display = "persp", col = "steelblue", border = NA,
    xlab = "\nmu", ylab = "\nsigma", zlab = "\n\np(mu, sigma)",
    shade = 0.8, phi = 30, ticktype = "detailed",
    cex.lab = 1.2, family = "Helvetica")
```

```{r plot-prior-2d-knitr, eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 6, fig.cap = "blah blah..."}
knitr::include_graphics("figures/prior.png")
```

## Échantillonner à partir du prior

```{r plot-prior-sigma, eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 6, fig.cap = "blah blah..."}
sample_mu <- rnorm(1000, 178, 20)
sample_sigma <- runif(1000, 0, 50)

data.frame(x = rnorm(1000, sample_mu, sample_sigma) ) %>%
    ggplot(aes(x) ) +
    geom_histogram() +
    xlab(expression(y[i]) ) +
    theme_bw(base_size = 20)
```

## Fonction de vraisemblance

```{r eval = TRUE, echo = TRUE}
mu_exemple <- 151.23
sigma_exemple <- 23.42

d2$height[34] # one observation
```

```{r likelihood-plot, eval = TRUE, echo = FALSE, fig.width = 6, fig.height = 6, fig.cap = "blah blah..."}
ggplot(data.frame(x = c(50, 250) ), aes(x) ) +
    stat_function(
        fun = dnorm, args = list(mu_exemple, sigma_exemple), lwd = 2) +
    geom_segment(
        aes(
            x = d2$height[34],
            xend = d2$height[34],
            y = 0,
            yend = dnorm(d2$height[34], mu_exemple,sigma_exemple) ),
        color = "black", size = 1, linetype = 2) +
    geom_point(
        data = d2,
        aes(x = d2$height[34], y = dnorm(d2$height[34], mu_exemple,sigma_exemple) ),
        size = 4) +
    xlab("Height") +
    ylab("Likelihood") +
    theme_bw(base_size = 20)
```

On veut calculer la probabilité d'observer une certaine valeur de taille, sachant certaines valeurs de $\mu$ et $\sigma$, c'est à dire :

$$
p(x \ | \ \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg[-\frac{1}{2 \sigma^{2}} (\mu - x)^{2} \bigg]
$$

On peut calculer cette *densité de probabilité* à l'aide des fonctions `dnorm`, `dbeta`, `dt`, `dexp`, `dgamma`, etc.

```{r eval = TRUE, echo = TRUE, fig.align = "center", fig.cap = "blah blah..."}
dnorm(d2$height[34], mu_exemple, sigma_exemple)
```

$$
p(x \ | \ \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg[-\frac{1}{2 \sigma^{2}} (\mu - x)^{2} \bigg]
$$

Ou à la main...

```{r eval = TRUE, echo = TRUE}
normal_likelihood <- function (x, mu, sigma) {
  
  bell <- exp( (- 1 / (2 * sigma^2) ) * (mu - x)^2 )
  norm <- sqrt(2 * pi * sigma^2)
  
  return(bell / norm)
  
}
```

```{r eval = TRUE, echo = TRUE}
normal_likelihood(d2$height[34], mu_exemple, sigma_exemple)
```

## Distribution postérieure

$$
\color{purple}{p(\mu, \sigma \ | \ h)} = \frac{\prod_{i} \color{orangered}{\mathrm{Normal}(h_{i} \ | \ \mu, \sigma)}\color{steelblue}{\mathrm{Normal}(\mu \ | \ 178, 20)\mathrm{Uniform}(\sigma \ | \ 0, 50)}}
{\color{green}{\int \int \prod_{i} \mathrm{Normal}(h_{i} \ | \ \mu, \sigma)\mathrm{Normal}(\mu \ | \ 178, 20)\mathrm{Uniform}(\sigma \ | \ 0, 50) \mathrm{d} \mu \mathrm{d} \sigma}}
$$

$$
\color{purple}{p(\mu, \sigma \ | \ h)} \propto \prod_{i} \color{orangered}{\mathrm{Normal}(h_{i} \ | \ \mu, \sigma)}\color{steelblue}{\mathrm{Normal}(\mu \ | \ 178, 20)\mathrm{Uniform}(\sigma \ | \ 0, 50)}
$$

Il s'agit de la même formule vue lors des cours 1 et 2, mais cette fois en considérant qu'il existe plusieurs observations de taille ($h_{i}$), et deux paramètres à estimer $\mu$ et $\sigma$.

Pour calculer la **vraisemblance marginale** (en vert), il faut donc intégrer sur deux paramètres : $\mu$ et $\sigma$.

On réalise ici encore que la probabilité a posteriori est proportionnelle au produit de la vraisemblance et du prior.

### Distribution postérieure - grid approximation

```{r grid, eval = TRUE, echo = TRUE}
# définit une grille de valeurs possibles pour mu et sigma
mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from = 4, to = 9, length.out = 200)

# étend la grille en deux dimensions (chaque combinaison de mu et sigma)
post <- expand.grid(mu = mu.list, sigma = sigma.list)

# calcul de la log-vraisemblance (pour chaque couple de mu et sigma)
post$LL <-
  sapply(
    1:nrow(post),
    function(i) sum(dnorm(
      d2$height,
      mean = post$mu[i],
      sd = post$sigma[i],
      log = TRUE)
      )
    )

# calcul de la probabilité a posteriori (non normalisée)
post$prod <-
  post$LL +
  dnorm(post$mu, 178, 20, log = TRUE) +
  dunif(post$sigma, 0, 50, log = TRUE)

# on "annule" le log en avec exp() et on standardise par la valeur maximale
post$prob <- exp(post$prod - max(post$prod) )
```

```{r sampling-posterior, eval = TRUE, echo = TRUE}
sample.rows <- sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)
```

```{r plotting-samples, eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 12, fig.height = 8, fig.cap = "blah blah..."}
library(viridis)

sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

ggplot(
    data.frame(sample.mu, sample.sigma),
    aes(x = sample.mu, y = sample.sigma)
    ) + 
    stat_density_2d(
        geom = "raster", aes(fill = ..density..),
        contour = FALSE, show.legend = FALSE
      ) +
    geom_vline(xintercept = mean(sample.mu), lty = 2) +
    geom_hline(yintercept = mean(sample.sigma), lty = 2) +
    scale_fill_viridis(na.value = "black") +
    coord_cartesian(
      xlim = c(min(sample.mu),max(sample.mu) ),
      ylim = c(min(sample.sigma),max(sample.sigma) )
      ) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) +
    labs(x = expression(mu), y = expression(sigma) ) +
    theme_bw(base_size = 20)
```

### Distribution postérieure - distributions marginales

```{r eval = TRUE, echo = TRUE, fig.cap = "blah blah..."}
BEST::plotPost(
  sample.mu, breaks = 40, xlab = expression(mu)
  )
```

```{r eval = TRUE, echo = TRUE, fig.cap = "blah blah..."}
BEST::plotPost(
  sample.sigma, breaks = 40, xlab = expression(sigma)
  )
```

## Introduction à brms

Under the hood : `Stan` est un langage de programmation probabiliste écrit en `C++`, et qui implémente plusieurs algorithmes de MCMC: HMC, NUTS, L-BFGS...

```{r stan, eval = FALSE, echo = TRUE}
data {
  int<lower=0> J; // number of schools 
  real y[J]; // estimated treatment effects
  real<lower=0> sigma[J]; // s.e. of effect estimates 
}

parameters {
  real mu; 
  real<lower=0> tau;
  real eta[J];
}

transformed parameters {
  real theta[J];
  for (j in 1:J)
    theta[j] = mu + tau * eta[j];
}

model {
  target += normal_lpdf(eta | 0, 1);
  target += normal_lpdf(y | theta, sigma);
}
```

Le package `brms` ([Bürkner, 2017](https://www.jstatsoft.org/article/view/v080i01)) permet de fitter des modèles multi-niveaux (ou pas) linéaires (ou pas) bayésiens en `Stan` mais en utilisant la syntaxe de `lme4`.

Par exemple, le modèle suivant :

$$
\begin{aligned}
y_{i} &\sim \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \alpha_{subject[i]} + \alpha_{item[i]} + \beta x_{i} \\
\end{aligned}
$$

se spécifie avec `brms` (comme avec `lme4`) de la manière suivante :

```{r eval = FALSE, echo = TRUE}
brm(y ~ x + (1 | subject) + (1 | item), data = d, family = gaussian() )
```

### Rappels de syntaxe

Le package `brms` utilise la même syntaxe que les fonctions de base R (comme `lm`) ou que le package `lme4`.

```{r eval = FALSE, echo = TRUE}
Reaction ~ Days + (1 + Days | Subject)
```

La partie gauche représente notre variable dépendante (ou *outcome*, i.e., ce qu'on essaye de prédire). Le package `brms` permet également de fitter des modèles multivariés (plusieurs outcomes) en les combinant avec `mvbind()`:

```{r eval = FALSE, echo = TRUE}
mvbind(Reaction, Memory) ~ Days + (1 + Days | Subject)
```

La partie droite permet de définir les prédicteurs. L'intercept est généralement implicite, de sorte que les deux écritures ci-dessous sont équivalentes.

```{r eval = FALSE, echo = TRUE}
mvbind(Reaction, Memory) ~ Days + (1 + Days | Subject)
mvbind(Reaction, Memory) ~ 1 + Days + (1 + Days | Subject)
```

Si l'on veut fitter un modèle sans intercept (why not), il faut le spécifier explicitement comme ci-dessous.

```{r eval = FALSE, echo = TRUE}
mvbind(Reaction, Memory) ~ 0 + Days + (1 + Days | Subject)
```

Par défaut `brms` postule une vraisemblance gaussienne. Ce postulat peut être changé facilement en spécifiant la vraisemblance souhaitée via l'argument `family`.

```{r eval = FALSE, echo = TRUE}
brm(Reaction ~ 1 + Days + (1 + Days | Subject), family = lognormal() )
```

Lisez la documentation (c'est très enthousiasmant à lire) accessible via `?brm`.

### Quelques fonctions utiles

```{r eval = FALSE, echo = TRUE}
# Generate the Stan code:
make_stancode(formula, ...)
stancode(fit)

# Generate the data passed to Stan:
make_standata(formula, ...)
standata(fit)

# Handle priors:
get_prior(formula, ...)
set_prior(prior, ...)

# Generate expected values and predictions:
fitted(fit, ...)
predict(fit, ...)
marginal_effects(fit, ...)

# Model comparison:
loo(fit1, fit2, ...)
bayes_factor(fit1, fit2, ...)
model_weights(fit1, fit2, ...)

# Hypothesis testing:
hypothesis(fit, hypothesis, ...)
```

### Un premier exemple

```{r mod1, eval = TRUE, echo = TRUE, results = "hide"}
library(brms)
mod1 <- brm(height ~ 1, data = d2)
```

```{r summary-mod1, eval = TRUE, echo = TRUE}
rbind(summary(mod1)$fixed, summary(mod1)$spec_pars )
```

Ces données représentent les distributions marginales de chaque paramètre. En d'autres termes, la *probabilité* de chaque valeur de $\mu$, après avoir *moyenné* sur toutes les valeurs possible de $\sigma$, est décrite par une distribution gaussienne avec une moyenne de $154.61$ et un écart type de $0.41$. L'intervalle de crédibilité ($\neq$ intervalle de confiance) nous indique les 95% valeurs de $\mu$ ou $\sigma$ les plus probables (sachant les données et les priors).

### En utilisant notre prior

Par défaut `brms` utilise un prior très peu informatif centré sur la valeur moyenne de la variable mesurée. On peut donc affiner l'estimation réalisée par ce modèle en utilisant nos connaissances sur la distribution habituelle des tailles chez les humains.

La fonction `get_prior()` permet de visualiser une liste des priors par défaut ainsi que de tous les prios qu'on peut spécifier, sachant une certaine formule (i.e., une manière d'écrire notre modèle) et un jeu de données.

```{r get-prior, eval = TRUE, echo = TRUE}
get_prior(height ~ 1, data = d2)
```

...

```{r mod2, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(178, 20), class = Intercept),
  prior(exponential(0.01), class = sigma)
  )

mod2 <- brm(
  height ~ 1,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

```{r prior-mod2, echo = FALSE, fig.width = 15, fig.height = 5, fig.cap = "blah blah..."}
library(patchwork)

p1 <- data.frame(x = c(100, 250) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dnorm, args = list(mean = 178, sd = 20),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  theme_bw(base_size = 20) +
  labs(x = expression(mu), y = "Probability density")

p2 <- data.frame(x = c(0, 500) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dexp, args = list(0.01),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  theme_bw(base_size = 20) +
  labs(x = expression(sigma), y = "Probability density")

p1 + p2
```

...

```{r summary-mod2, eval = TRUE, echo = TRUE}
summary(mod2)
```

### En utilisant un prior plus informatif

```{r mod3, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(178, 0.1), class = Intercept),
  prior(exponential(0.01), class = sigma)
  )

mod3 <- brm(
  height ~ 1,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

```{r prior-mod3, echo = FALSE, fig.width = 15, fig.height = 5, fig.cap = "blah blah..."}
library(patchwork)

p1 <- data.frame(x = c(177, 179) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dnorm, args = list(mean = 178, sd = 0.1),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  theme_bw(base_size = 20) +
  labs(x = expression(mu), y = "Probability density")

p2 <- data.frame(x = c(0, 500) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dexp, args = list(0.01),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  theme_bw(base_size = 20) +
  labs(x = expression(sigma), y = "Probability density")

p1 + p2
```

...

```{r summary-mod3, eval = TRUE, echo = TRUE}
summary(mod3)
```

On remarque que la valeur estimée pour $\mu$ n'a presque pas "bougée" du prior...mais on remarque également que la valeur estimée pour $\sigma$ a largement augmentée. Nous avons dit au modèle que nous étions assez certain de notre valeur de $\mu$, le modèle s'est ensuite "adapté", ce qui explique la valeur de $\sigma$...

### Précision du prior (heuristique)

Le prior peut généralement être considéré comme un posterior obtenu sur des données antérieures.

On sait que le $\sigma$ d'un posterior gaussien nous est donné par la formule:

$$\sigma_{post} = 1 / \sqrt{n}$$

Qui implique une *quantité de données* $n = 1 / \sigma^2_{post}$. Notre prior avait un $\sigma = 0.1$, ce qui donne $n = 1 / 0.1^2 = 100$.

Donc, on peut considérer que le prior $\mu \sim \mathrm{Normal}(178, 0.1)$ est équivalent au cas dans lequel nous aurions observé $100$ tailles de moyenne $178$.

### Récupérer et visualiser les échantillons de la distribution postérieure

```{r get-density-function, eval = TRUE, echo = FALSE}
library(viridis)
library(MASS)

# Get density of points in 2 dimensions.
# @param x A numeric vector.
# @param y A numeric vector.
# @param n Create a square n by n grid to compute density.
# @return The density within each square.

get_density <- function(x, y, n = 100) {
    
    dens <- MASS::kde2d(x = x, y = y, n = n)
    ix <- findInterval(x, dens$x)
    iy <- findInterval(y, dens$y)
    ii <- cbind(ix, iy)
    return(dens$z[ii])
    
}
```

```{r samples-plot, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 6, fig.cap = "blah blah..."}
post <- posterior_samples(mod2) %>%
    mutate(density = get_density(b_Intercept, sigma, n = 1e2) )

ggplot(post, aes(x = b_Intercept, y = sigma, color = density) ) +
    geom_point(size = 2, alpha = 0.5, show.legend = FALSE) +
    theme_bw(base_size = 20) +
    labs(x = expression(mu), y = expression(sigma) ) +
    viridis::scale_color_viridis()
```

### Récupérer les échantillons de la distribution postérieure

```{r eval = TRUE, echo = TRUE}
# gets the first 6 samples
head(post)

# gets the median and the 95% credible interval
t(sapply(post[, 1:2], quantile, probs = c(0.025, 0.5, 0.975) ) )
```

### Visualiser la distribution postérieure

```{r eval = FALSE, echo = TRUE}
H.scv <- Hscv(post[, 1:2])
fhat_post <- kde(x = post[, 1:2], H = H.scv, compute.cont = TRUE)

plot(fhat_post, display = "persp", col = "purple", border = NA,
  xlab = "\nmu", ylab = "\nsigma", zlab = "\np(mu, sigma)",
  shade = 0.8, phi = 30, ticktype = "detailed",
  cex.lab = 1.2, family = "Helvetica")
```

```{r eval = TRUE, echo = TRUE, fig.cap = "blah blah..."}
knitr::include_graphics("figures/posterior.png")
```

### Visualiser la distribution postérieure

```{r plot-samples, eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 12, fig.height = 8, fig.cap = "blah blah..."}
library(viridis)

sample.mu <- post$b_Intercept
sample.sigma <- post$sigma

data.frame(sample.mu, sample.sigma) %>%
    ggplot(aes(x = sample.mu, y = sample.sigma) ) + 
    stat_density_2d(
        geom = "raster",
        aes(fill = ..density..),
        contour = FALSE, show.legend = FALSE
        ) +
    geom_vline(xintercept = mean(sample.mu), lty = 2) +
    geom_hline(yintercept = mean(sample.sigma), lty = 2) +
    scale_fill_viridis(na.value = "black") +
    coord_cartesian(
        xlim = c(min(sample.mu), max(sample.mu) ),
        ylim = c(min(sample.sigma), max(sample.sigma) )
        ) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) +
    labs(x = expression(mu), y = expression(sigma) ) +
    theme_bw(base_size = 20)
```

### Ajouter un prédicteur

Comment est-ce que la taille co-varie avec le poids ?

```{r height-weight-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6, fig.cap = "blah blah..."}
d2 %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  theme_bw(base_size = 20)
```

## Régression linéaire à un prédicteur continu

$$
\begin{aligned}
h_{i} &\sim \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta x_{i} \\
\end{aligned}
$$

```{r lm-regression, eval = TRUE, echo = TRUE}
linear_model <- lm(height ~ weight, data = d2)
precis(linear_model, prob = 0.95)
```

```{r lm-regression-plot, eval = TRUE, echo = FALSE, fig.width = 7.5, fig.height = 5, fig.cap = "blah blah..."}
d2 %>%
    ggplot(aes(x = weight, y = height) ) +
    geom_point(
      colour = "white", fill = "black",
      pch = 21, size = 3, alpha = 0.8
      ) +
    geom_smooth(method = "lm", se = FALSE, color = "black", lwd = 1) +
    theme_bw(base_size = 20)
```

### Différentes notations équivalentes

On considère un modèle de régression linéaire avec un seul prédicteur, une pente, un intercept, et des résidus distribués selon une loi normale. La notation :

$$
h_{i} = \alpha + \beta x_{i} + \epsilon_{i} \quad \text{avec} \quad \epsilon_{i} \sim \mathrm{Normal}(0, \sigma)
$$

est équivalente à :

$$
h_{i} - (\alpha + \beta x_{i}) \sim \mathrm{Normal}(0, \sigma)
$$

et si on réduit encore un peu :

$$
h_{i} \sim \mathrm{Normal}(\alpha + \beta x_{i}, \sigma).
$$

Les notations ci-dessus sont équivalentes, mais la dernière est plus flexible, et nous permettra par la suite de l'étendre plus simplement aux modèles multi-niveaux.

$$
\begin{aligned}
\color{orangered}{h_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i},\sigma)} \\
\mu_{i} &= \alpha + \beta x_{i} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal}(178, 20)} \\
\color{steelblue}{\beta} \ &\color{steelblue}{\sim \mathrm{Normal}(0, 10)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{Exponential}(0.01)} \\
\end{aligned}
$$

Dans ce modèle $\mu$ n'est plus un paramètre à estimer (car $\mu$ est *déterminé* par $\alpha$ et $\beta$). À la place, nous allons estimer $\alpha$ et $\beta$.

Rappels: $\alpha$ est l'*intercept*, c'est à dire la taille attendue, lorsque le poids est égal à $0$. $\beta$ est la pente, c'est à dire le changement de taille attendu quand le poids augmente d'une unité.

```{r mod4, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(178, 20), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(exponential(0.01), class = sigma)
  )

mod4 <- brm(
  height ~ 1 + weight,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

```{r summary-mod4, eval = TRUE, echo = TRUE}
summary(mod4)
```

- $\beta = 0.90, 95\% \ \text{CrI} \ [0.82, 0.99]$ nous indique qu'une augmentation de 1kg entraîne une augmentation de 0.90cm.
- $\alpha = 113.91, 95\% \ \text{CrI} \ [110.12, 117.59]$ représente la taille moyenne quand le poids est égal à 0kg...

...

```{r mod5, eval = TRUE, echo = TRUE, results = "hide"}
d2$weight.c <- d2$weight - mean(d2$weight)

mod5 <- brm(
  height ~ 1 + weight.c,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

```{r fixef-mod5, eval = TRUE, echo = TRUE}
fixef(mod5) # retrieves the fixed effects estimates
```

- Après avoir centré la réponse, l'intercept représente la valeur attendue de *taille* lorsque le poids est à sa valeur moyenne.

### Représenter les prédictions du modèle

```{r mod4-predictions, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 6, fig.cap = "blah blah..."}
d2 %>%
    ggplot(aes(x = weight, y = height) ) +
    geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
    geom_abline(intercept = fixef(mod4)[1], slope = fixef(mod4)[2], lwd = 1) +
    theme_bw(base_size = 20)
```

### Représenter l'incertitude sur $\mu$ via fitted()

```{r fitted-mod4, eval = TRUE, echo = TRUE}
# on crée un vecteur de valeurs possibles pour "weight"
weight.seq <- data.frame(weight = seq(from = 25, to = 70, by = 1) )

# on récupère les prédictions du modèle pour ces valeurs de poids
mu <- data.frame(fitted(mod4, newdata = weight.seq) ) %>% bind_cols(weight.seq)

# on affiche les 10 premières lignes de mu
head(mu, 10)
```

...

```{r fitted-mod4-plot, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 5, fig.cap = "blah blah..."}
d2 %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
    stat = "identity",
    color = "black", alpha = 0.8, size = 1
    ) +
  theme_bw(base_size = 20)
```

### Intervalles de prédiction (incorporer $\sigma$)

Pour rappel, voici notre modèle : $h_{i} \sim \mathrm{Normal}(\alpha + \beta x_{i}, \sigma)$. Pour l'instant, on a seulement représenté les prédictions pour $\mu$. Comment incorporer $\sigma$ dans nos prédictions ?

```{r predict-mod4, eval = TRUE, echo = TRUE}
# on crée un vecteur de valeurs possibles pour "weight"
weight.seq <- data.frame(weight = seq(from = 25, to = 70, by = 1) )

# on récupère les prédictions du modèle pour ces valeurs de poids
pred_height <- data.frame(predict(mod4, newdata = weight.seq) ) %>% bind_cols(weight.seq)

# on affiche les 10 premières lignes de pred_height
head(pred_height, 10)
```

```{r predict-mod4-plot, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 4, fig.cap = "blah blah..."}
d2 %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_ribbon(
    data = pred_height, aes(x = weight, ymin = Q2.5, ymax = Q97.5),
    alpha = 0.2, inherit.aes = FALSE
    ) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
    stat = "identity", color = "black", alpha = 0.8, size = 1
    ) +
  theme_bw(base_size = 20)
```

### Deux types d'incertitude

Deux sources d'incertitude dans le modèle : incertitude concernant l'estimation de la valeur des paramètres mais également concernant le processus d'échantillonnage.

**Incertitude épistémique** : La distribution a posteriori ordonne toutes les combinaisons possibles des valeurs des paramètres selon leurs plausibilités relatives.

**Incertitude aléatoire** : La distribution des données simulées est elle, une distribution qui contient de l'incertitude liée à un processus d'échantillonnage (i.e., générer des données à partir d'une gaussienne).

Voir aussi ce [court article](http://www.stat.columbia.edu/~gelman/stuff_for_blog/ohagan.pdf) par O'Hagan (2012).

## Régression polynomiale

```{r plot-poly, eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 4, fig.cap = "blah blah..."}
d %>% # on utilise d au lieu de d2
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  theme_bw(base_size = 20)
```

Si on considère tout l'échantillon (pas seulement les adultes), la relation entre taille et poids semble incurvée...

```{r poly-plot-std, eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 4, fig.cap = "blah blah..."}
d <- d %>% mutate(weight.s = (weight - mean(weight) ) / sd(weight) )

d %>%
    ggplot(aes(x = weight.s, y = height) ) +
    geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
    theme_bw(base_size = 20)

c(mean(d$weight.s), sd(d$weight.s) )
```

Pourquoi standardiser les prédicteurs ?

- **Interprétation**. Un changement d'une unité du prédicteur correspond à un changement d'un écart-type sur la réponse. Permet de comparer les coefficients de plusieurs prédicteurs.
- **Fitting**. Quand les prédicteurs contiennent de grandes valeurs, cela peut poser des problèmes...

### Modèle de régression polynomiale

$$
\begin{aligned}
&\color{orangered}{h_{i} \sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
&\mu_{i} = \alpha + \beta_{1} x_{i} + \beta_{2} x_{i}^{2} \\
&\color{steelblue}{\alpha \sim \mathrm{Normal}(156, 100)} \\
&\color{steelblue}{\beta_{1} \sim \mathrm{Normal}(0, 10)} \\
&\color{steelblue}{\beta_{2} \sim \mathrm{Normal}(0, 10)} \\
&\color{steelblue}{\sigma \sim \mathrm{Exponential}(0.01)} \\
\end{aligned}
$$

À vous de construire ce modèle en utilisant `brms::brm()`...

```{r mod6, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(156, 100), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(exponential(0.01), class = sigma)
  )

mod6 <- brm(
  # NB: polynomials should be written with the I() function...
  height ~ 1 + weight.s + I(weight.s^2),
  prior = priors,
  family = gaussian(),
  data = d
  )
```

```{r summary-mod6, eval = TRUE, echo = TRUE}
summary(mod6)
```

...

### Représenter les prédictions du modèle

```{r predict-mod6, eval = TRUE, echo = TRUE}
# on crée un vecteur de valeurs possibles pour "weight"
weight.seq <- data.frame(weight.s = seq(from = -2.5, to = 2.5, length.out = 50) )

# on récupère les prédictions du modèle pour ces valeurs de poids
mu <- data.frame(fitted(mod6, newdata = weight.seq) ) %>% bind_cols(weight.seq)
pred_height <- data.frame(predict(mod6, newdata = weight.seq) ) %>% bind_cols(weight.seq)

# on affiche les 10 premières lignes de pred_height
head(pred_height, 10)
```

...

```{r predict-mod6-plot, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 5, fig.cap = "blah blah..."}
d %>%
  ggplot(aes(x = weight.s, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_ribbon(
    data = pred_height, aes(x = weight.s, ymin = Q2.5, ymax = Q97.5),
    alpha = 0.2, inherit.aes = FALSE
    ) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
    stat = "identity", color = "black", alpha = 0.8, size = 1
    ) +
  theme_bw(base_size = 20)
```

## Modèle de régression, taille d'effet

Il existe plusieurs méthodes pour calculer les tailles d'effet dans les modèles bayésiens. [Gelman & Pardoe (2006)](http://www.stat.columbia.edu/~gelman/research/published/rsquared.pdf) proposent une méthode pour calculer un $R^{2}$ basé sur l'échantillon.

[Marsman et al. (2017)](http://rsos.royalsocietypublishing.org/content/4/1/160426), [Marsman et al. (2019)](https://onlinelibrary.wiley.com/doi/full/10.1111/stan.12173) généralisent des méthodes existantes pour calculer un $\rho^{2}$ pour les designs de type ANOVA (i.e., avec prédicteurs catégoriels), qui représente une estimation de la taille d'effet *dans la population*, et non basé sur l'échantillon.

> *"Similar to most of the ES measures that have been proposed for the ANOVA model, the squared multiple correlation coefficient $\rho^{2}$ [...] is a so-called proportional reduction in error measure (PRE; Reynolds, 1977). In general, a PRE measure expresses the proportion of the variance in an outcome $y$ that is attributed to the independent variables $x$*" ([Marsman et al., 2019](https://onlinelibrary.wiley.com/doi/full/10.1111/stan.12173)).

$$
\begin{aligned}
\rho^{2} &= \dfrac{\sum_{i = 1}^{n} \pi_{i}(\beta_{i} - \beta)^{2}}{\sigma^{2} + \sum_{i=1}^{n} \pi_{i}(\beta_{i} - \beta)^{2}} \\  \rho^{2} &= \dfrac{ \frac{1}{n} \sum_{i=1}^{n} \beta_{i}^{2}}{\sigma^{2} + \frac{1}{n} \sum_{i = 1}^{n} \beta_{i}^{2}} \\ \rho^{2} &= \dfrac{\beta^{2} \tau^{2}}{\sigma^{2} + \beta^{2} \tau^{2}}\\
\end{aligned}
$$

```{r effsize, eval = TRUE, echo = TRUE}
post <- posterior_samples(mod4)
beta <- post$b_weight
sigma <- post$sigma

f1 <- beta^2 * var(d2$weight)
rho <- f1 / (f1 + sigma^2)
```

Attention, si plusieurs prédicteurs, dépend de la structure de covariance...

```{r effsize-BEST-plot, eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 5, fig.cap = "blah blah..."}
BEST::plotPost(rho, showMode = TRUE, xlab = expression(rho) )
```

```{r summary-lm-effsize, eval = TRUE, echo = TRUE}
summary(lm(height ~ weight, data = d2) )$r.squared
```

## Conclusions

On a présenté un nouveau modèle à deux puis trois paramètres : le modèle gaussien, puis la régression linéaire gaussienne, permettant de mettre en relation deux variables continues.

Comme précédemment, le théorème de Bayes est utilisé pour mettre à jour nos connaissances a priori quant à la valeur des paramètres en une connaissance a posteriori, synthèse entre nos priors et l'information contenue dans les données.

La package `brms` permet de fitter toutes sortes de modèles avec une syntaxe similaire à celle utilisée par `lm()`.

La fonction `fitted()` permet de récupérer les prédictions d'un modèle fitté avec `brms` (i.e., un modèle de classe `brmsfit`).

La fonction `predict()` permet de simuler des données à partir d'un modèle fitté avec `brms`.
