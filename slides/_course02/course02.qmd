---
title: Introduction to Bayesian statistical modelling
subtitle: A course with R, Stan, and brms
author: Ladislas Nalborczyk (UNICOG, NeuroSpin, CEA, Gif/Yvette, France)
from: markdown+emoji
format:
  revealjs:
    incremental: true
    theme: [default, ../custom.scss]
    transition: none # fade
    background-transition: none # fade
    transition-speed: default
    slide-number: c/t
    show-slide-number: all
    preview-links: true
    self-contained: true # when sharing slides
    # chalkboard: true
    csl: ../../files/bib/apa7.csl
    logo: ../../files/cover.png
    footer: "Ladislas Nalborczyk - IBSM2023"
    # width: 1200 # defaults to 1050
    # height: 900 # default to 700
    margin: 0.15 # defaults to 0.1
    scrollable: true
    hide-inactive-cursor: true
    pdf-separate-fragments: false
    highlight-style: zenburn
    code-copy: true
    code-link: false
    code-fold: false
    code-summary: "See the code"
    numbers: true
    progress: false
title-slide-attributes:
    data-background-color: "#1c5253"
bibliography: ../../files/bib/references.bib
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

```{r setup, eval = TRUE, include = FALSE, cache = FALSE}
library(countdown)
library(tidyverse)
library(knitr)

# setting up knitr options
knitr::opts_chunk$set(
  cache = TRUE, echo = TRUE,
  warning = FALSE, message = FALSE,
  fig.align = "center", dev = "svg"
  )

# setting up ggplot theme
theme_set(theme_bw(base_size = 16, base_family = "Open Sans") )
```

## Planning

Course n°01: Introduction to Bayesian inference, Beta-Binomial model <br> **Course n°02: Introduction to brms, linear regression** <br> Course n°03: Markov Chain Monte Carlo, generalised linear model <br> Course n°04: Multilevel models, cognitive models <br>

$$\newcommand\given[1][]{\:#1\vert\:}$$

## A common language

$$
\begin{align}
y_{i} &\sim \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i}&= \alpha + \beta x_{i} \\
\alpha &\sim \mathrm{Normal}(60, 10) \\
\beta &\sim \mathrm{Normal}(0, 10) \\
\sigma &\sim \mathrm{HalfCauchy}(0, 1)
\end{align}
$$

**Aim of this session**: to understand this type of model.

. . .

The components of our models will always be the same and we will always follow the following three steps:

- Build the model (likelihood + priors).
- Update with data to calculate the posterior distribution.
- Interpret the model's estimates and evaluate its predictions. If necessary, modify the model.

# A first model


## A first model

```{r echo = TRUE}
library(tidyverse)
library(imsb)

d <- open_data(howell)
str(d)
```

. . .

```{r echo = TRUE}
d2 <- d %>% filter(age >= 18)
head(d2)
```

## A first model

$$h_{i} \sim \mathrm{Normal}(\mu, \sigma)$$

```{r echo = TRUE, fig.width = 6, fig.height = 6}
d2 %>%
    ggplot(aes(x = height) ) +
    geom_histogram(aes(y = ..density..), bins = 20, col = "white") +
    stat_function(fun = dnorm, args = list(mean(d2$height), sd(d2$height) ), size = 1)
```

## Normal distribution

$$
p(x \given \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg[-\frac{1}{2 \sigma^{2}} (\mu - x)^{2} \bigg]
$$

```{r eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 6}
data.frame(value = rnorm(n = 1e4, mean = 10, sd = 1) ) %>% # 10.000 samples from Normal(10, 1)
    ggplot(aes(x = value) ) +
    geom_histogram(col = "white")
```

## Where does the normal distribution come from?

Some constraints: Some values are highly probable (around the mean $\mu$). The further away they are, the less likely they are (following an exponential decay).

```{r normal-explain1, echo = FALSE, fig.width = 12, fig.height = 6}
f1 <- function(x) {exp(-x)}
f2 <- function(x) {exp(-x^2)}

ggplot(data.frame(x = c(0, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f1, lwd = 1) +
  geom_path(aes(colour = "orangered"), stat = "function", fun = f2, lwd = 1) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-x)", "y = exp(-x^2)"),
    breaks = c("steelblue", "orangered")
    )
```

## Where does the normal distribution come from?

$$
y = \exp \big[-x^{2} \big]
$$

We extend our function to negative values.

```{r normal-explain2, echo = FALSE, fig.width = 12, fig.height = 6}
ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f2, lwd = 1) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-x^2)", "y = exp(-x^2)"),
    breaks = c("steelblue", "orangered")
    )
```

## Where does the normal distribution come from?

$$
y = \exp \big[-x^{2} \big]
$$

The inflection points give us a good indication of where most of the values
lie (i.e., between the inflection points). The peaks of the derivative show us the inflection points.

```{r normal-explain3, echo = FALSE, fig.width = 12, fig.height = 6}
f <- expression(exp(-x^2) )
f_derivative <- D(f, "x")

f3 <- function(x) {-(exp(-x^2) * (2 * x) )}
f3_min <- optimize(f = f3, interval = c(-10, 10) )$minimum

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f2, lwd = 1) +
  geom_area(
    data = data.frame(x = seq(-5, 5, 0.01) ) %>% filter(-f3_min < x & x < f3_min),
    aes(y = exp(-x^2) ),
    position = "identity", fill = "steelblue", alpha = 0.3
    ) +
  geom_path(aes(colour = "orangered"), stat = "function", fun = f3, lwd = 1) +
  geom_vline(xintercept = -f3_min, colour = "orangered", lty = 2) +
  geom_vline(xintercept = f3_min, colour = "orangered", lty = 2) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-x^2)", "derivative"),
    breaks = c("steelblue", "orangered")
    )
```

## Where does the normal distribution come from?

$$
y = \exp \bigg [- \frac{1}{2} x^{2} \bigg]
$$

Next, we standardise the distribution so that the two inflection points are located at $x = -1$ and $x = 1$.

```{r normal-explain4, echo = FALSE, fig.width = 12, fig.height = 6}
f <- expression(exp((-0.5) * x^2) )
f_derivative <- D(f, "x")

f4 <- function(x) {exp((-0.5) * x^2)}
f5 <- function(x) {exp((-0.5) * x^2) * ((-0.5) * (2 * x) )}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(aes(colour = "steelblue"), stat = "function", fun = f4, lwd = 1) +
  geom_path(aes(colour = "orangered"), stat = "function", fun = f5, lwd = 1) +
  geom_vline(xintercept = -1, colour = "orangered", lty = 2) +
  geom_vline(xintercept = 1, colour = "orangered", lty = 2) +
  labs(x = "x", y = "f(x)") +
  scale_colour_identity(
    "Function", guide = "legend",
    labels = c("y = exp(-0.5x^2)", "derivative"),
    breaks = c("steelblue", "orangered")
    )
```

## Where does the normal distribution come from?

$$
y = \exp \bigg [- \frac{1}{2 \color{steelblue}{\sigma^{2}}} x^{2} \bigg]
$$

We then add a parameter $\sigma^{2}$ to control the distance between the inflection points.

```{r normal-explain5, echo = FALSE, fig.width = 12, fig.height = 6}
f4 <- function(x) {exp((-0.5) * x^2)}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(
    aes(colour = "steelblue"),
    colour = "steelblue",
    stat = "function", fun = f4, lwd = 1
    ) +
  labs(x = "x", y = "f(x)")
```

## Where does the normal distribution come from?

$$
y = \exp \bigg [- \frac{1}{2 \color{steelblue}{\sigma^{2}}} (\color{orangered}{\mu} - x)^{2} \bigg]
$$

We then insert a parameter $\mu$ to control the position (central tendency) of the distribution.

```{r normal-explain6, echo = FALSE, fig.width = 12, fig.height = 6}
f4 <- function(x) {exp((-0.5) * (x - 3)^2)}

ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(
    aes(colour = "steelblue"),
    colour = "steelblue",
    stat = "function", fun = f4, lwd = 1
    ) +
  labs(x = "x", y = "f(x)")
```

## Where does the normal distribution come from?

$$
y = \frac{1}{\sqrt{2 \pi \color{steelblue}{\sigma^{2}}}} \exp \bigg[-\frac{1}{2 \color{steelblue}{\sigma^{2}}} (\color{orangered}{\mu} - x)^{2} \bigg]
$$

But... this distribution does not integrate to 1. We therefore divide by a normalisation constant (the left-hand side), to obtain a proper (valid) probability distribution.

```{r normal-explain7, echo = FALSE, fig.width = 12, fig.height = 6}
ggplot(data.frame(x = c(-5, 5) ), aes(x = x) ) + 
  geom_path(
    aes(colour = "steelblue"),
    colour = "steelblue",
    stat = "function", fun = dnorm, lwd = 1
    ) +
  labs(x = "x", y = "f(x)")
```

## Gaussian model

We are going to build a regression model, but before we add a predictor, let's try to model the distribution of heights.

. . .

We want to know which model (distribution) best describes the distribution of heights. We will therefore explore all the possible combinations of $\mu$ and $\sigma$ and rank them by their respective probabilities.

. . .

Our aim, once again, is to describe **the posterior distribution**, which will therefore be in some way **a distribution of distributions**.

## Gaussian model

We define $p(\mu, \sigma)$, the joint prior distribution of all model parameters. These priors can be specified independently for each parameter, given that $p(\mu, \sigma) = p(\mu) p(\sigma)$.

$$\color{steelblue}{\mu \sim \mathrm{Normal}(174, 20)}$$

```{r eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 9, fig.height = 6}
data.frame(x = c(100, 250) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dnorm, args = list(mean = 174, sd = 20),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  labs(x = expression(mu), y = "Probability density")
```

## Gaussian model

We define $p(\mu, \sigma)$, the joint prior distribution of all model parameters. These priors can be specified independently for each parameter, given that $p(\mu, \sigma) = p(\mu) p(\sigma)$.

$$\color{steelblue}{\sigma \sim \mathrm{Uniform}(0, 50)}$$

```{r eval = TRUE, echo = FALSE, fig.width = 9, fig.height = 6}
data.frame(x = c(-10, 60) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dunif, args = list(0, 50),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  labs(x = expression(sigma), y = "Probability density")
```

## Visualising the prior

```{r eval = FALSE, echo = TRUE}
library(ks)
sample_mu <- rnorm(1e4, 174, 20) # prior on mu
sample_sigma <- runif(1e4, 0, 50) # prior on sigma
prior <- data.frame(cbind(sample_mu, sample_sigma) ) # multivariate prior
H.scv <- Hscv(x = prior, verbose = TRUE)
fhat_prior <- kde(x = prior, H = H.scv, compute.cont = TRUE)
plot(
    fhat_prior, display = "persp", col = "steelblue", border = NA,
    xlab = "\nmu", ylab = "\nsigma", zlab = "\n\np(mu, sigma)",
    shade = 0.8, phi = 30, ticktype = "detailed",
    cex.lab = 1.2, family = "Helvetica")
```

```{r prior-plot, echo = FALSE, out.width = "500px"}
knitr::include_graphics("figures/prior.png")
```

## Prior predictive checking

```{r eval = TRUE, echo = TRUE, fig.width = 6, fig.height = 6}
sample_mu <- rnorm(1000, 174, 20)
sample_sigma <- runif(1000, 0, 50)

data.frame(x = rnorm(n = 1000, mean = sample_mu, sd = sample_sigma) ) %>%
    ggplot(aes(x) ) +
    geom_histogram() +
    labs(x = "Height (cm)", y = "Number of samples")
```

## Likelihood function

```{r eval = TRUE, echo = TRUE}
mu_exemple <- 151.23
sigma_exemple <- 23.42

d2$height[34] # exemplary height observation
```

```{r eval = TRUE, echo = FALSE, fig.width = 6, fig.height = 6}
ggplot(data.frame(x = c(50, 250) ), aes(x) ) +
    stat_function(
        fun = dnorm, args = list(mu_exemple, sigma_exemple), lwd = 2) +
    geom_segment(
        aes(
            x = d2$height[34],
            xend = d2$height[34],
            y = 0,
            yend = dnorm(d2$height[34], mu_exemple, sigma_exemple) ),
        color = "black", size = 1, linetype = 2) +
    geom_point(
        data = d2,
        aes(x = d2$height[34], y = dnorm(d2$height[34], mu_exemple,sigma_exemple) ),
        size = 4) +
    xlab("Height (cm)") +
    ylab("Likelihood")
```

## Likelihood function

We want to compute the probability of observing a certain value of height knowing certain values of $\mu$ and $\sigma$, that is:

$$
p(x \given \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg[-\frac{1}{2 \sigma^{2}} (\mu - x)^{2} \bigg]
$$

. . .

This **probability density** can be computed using the functions
`dnorm`, `dbeta`, `dt`, `dexp`, `dgamma`, etc.

```{r eval = TRUE, echo = TRUE}
dnorm(d2$height[34], mu_exemple, sigma_exemple)
```

## Likelihood function

$$
p(x \given \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg[-\frac{1}{2 \sigma^{2}} (\mu - x)^{2} \bigg]
$$

Or using a custom function...

```{r eval = TRUE, echo = TRUE}
normal_likelihood <- function (x, mu, sigma) {
  
  bell <- exp( (- 1 / (2 * sigma^2) ) * (mu - x)^2 )
  norm <- sqrt(2 * pi * sigma^2)
  
  return(bell / norm)
  
}
```

. . .

```{r eval = TRUE, echo = TRUE}
normal_likelihood(d2$height[34], mu_exemple, sigma_exemple)
```

## Posterior distribution

$$
\color{purple}{p(\mu, \sigma \given h)} = \frac{\prod_{i} \color{orangered}{\mathrm{Normal}(h_{i} \given \mu, \sigma)}\color{steelblue}{\mathrm{Normal}(\mu \given 174, 20)\mathrm{Uniform}(\sigma \given 0, 50)}}
{\color{green}{\int \int \prod_{i} \mathrm{Normal}(h_{i} \given \mu, \sigma)\mathrm{Normal}(\mu \given 174, 20)\mathrm{Uniform}(\sigma \given 0, 50) \mathrm{d} \mu \mathrm{d} \sigma}}
$$

$$
\color{purple}{p(\mu, \sigma \given h)} \propto \prod_{i} \color{orangered}{\mathrm{Normal}(h_{i} \given \mu, \sigma)}\color{steelblue}{\mathrm{Normal}(\mu \given 174, 20)\mathrm{Uniform}(\sigma \given 0, 50)}
$$

. . .

This is the same formula from the previous course, but here considering that there are several observations of height ($h_{i}$) and two parameters to be estimated: $\mu$ and $\sigma$.

. . .

To compute the **marginal likelihood** (in green), we therefore need to integrate over two parameters: $\mu$ and $\sigma$. Here again we realise that the posterior is proportional to the product of the likelihood and the prior.

## Posterior distribution - Grid approximation

```{r grid, eval = TRUE, echo = TRUE}
# we define a grid of possible values for mu and sigma
mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from = 4, to = 9, length.out = 200)

# we extend this grid to all possible combinations of mu and sigma
post <- expand.grid(mu = mu.list, sigma = sigma.list)

# we compute the log-likelihood for each observation (under each combination of mu and sigma)
post$LL <-
  sapply(
    1:nrow(post),
    function(i) sum(dnorm(
      d2$height,
      mean = post$mu[i],
      sd = post$sigma[i],
      log = TRUE) )
    )

# we compute the (unnormalised) posterior distribution
post$prod <-
  post$LL +
  dnorm(x = post$mu, mean = 174, sd = 20, log = TRUE) +
  dunif(x = post$sigma, min = 0, max = 50, log = TRUE)


# we "cancel" the log and we standardise by the maximum value (to avoid rounding errors)
post$prob <- exp(post$prod - max(post$prod) )
```

## Posterior distribution - Grid approximation

```{r samples1, eval = TRUE, echo = TRUE}
# randomly selecting 20 rows from the resulting dataframe 
post %>% slice_sample(n = 20, replace = FALSE)
```

## Posterior distribution - Grid approximation

```{r sampling-posterior, eval = TRUE, echo = TRUE}
sample.rows <- sample(x = 1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

```{r plotting-samples, eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 12, fig.height = 8}
library(viridis)

sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

ggplot(
    data.frame(sample.mu, sample.sigma),
    aes(x = sample.mu, y = sample.sigma)
    ) + 
    stat_density_2d(
        geom = "raster", aes(fill = ..density..),
        contour = FALSE, show.legend = FALSE
      ) +
    geom_vline(xintercept = mean(sample.mu), lty = 2) +
    geom_hline(yintercept = mean(sample.sigma), lty = 2) +
    scale_fill_viridis(na.value = "black") +
    coord_cartesian(
      xlim = c(min(sample.mu), max(sample.mu) ),
      ylim = c(min(sample.sigma), max(sample.sigma) )
      ) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) +
    labs(x = expression(mu), y = expression(sigma) )
```

## Posterior distribution - Marginal distributions

```{r eval = TRUE, echo = TRUE, dev = "png", dpi = 200}
posterior_plot(samples = sample.mu, nbins = 35) + labs(x = expression(mu) )
```

## Posterior distribution - Marginal distributions

```{r eval = TRUE, echo = TRUE, dev = "png", dpi = 200}
posterior_plot(samples = sample.sigma, nbins = 30) + labs(x = expression(sigma) )
```

# Introducing brms


## Introducing brms

Under the hood: `Stan` is a probabilistic programming language written in `C++`, which implements several MCMC algorithms: HMC, NUTS, L-BFGS...

```{r stan, eval = FALSE, echo = TRUE}
data {
  int<lower=0> J; // number of schools 
  real y[J]; // estimated treatment effects
  real<lower=0> sigma[J]; // s.e. of effect estimates 
  }

parameters {
  real mu; 
  real<lower=0> tau;
  real eta[J];
  }

transformed parameters {
  real theta[J];
  for (j in 1:J)
    theta[j] = mu + tau * eta[j];
  }

model {
  target += normal_lpdf(eta | 0, 1);
  target += normal_lpdf(y | theta, sigma);
  }
```

## Bayesian regression models using Stan

The `brms` package [@bürkner2017] can be used to fit multilevel (or single-level) linear (or not) Bayesian regression models in `Stan` but using the intuitive syntax of `lme4` [cf. our tutorial paper, @nalborczyk2019].

. . .

For instance, the following model:

$$
\begin{align}
y_{i} &\sim \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \alpha_{\text{subject}[i]} + \alpha_{\text{item}[i]} + \beta x_{i} \\
\end{align}
$$

. . .

is specified with `brms` (as with `lme4`) as follows:

```{r eval = FALSE, echo = TRUE}
model <- brm(y ~ x + (1 | subject) + (1 | item), data = d, family = gaussian() )
```

## Syntax reminders

The `brms` package uses the same syntax as the base `R` functions (such as `lm`) or functions from the `lme4` package.

```{r eval = FALSE, echo = TRUE}
Reaction ~ Days + (1 + Days | Subject)
```

. . .

The left-hand side represents our dependent variable (or outcome, that is, what we are trying to predict). The `brms` package can also be used to fit multivariate models (several outcomes) by combining them with with `mvbind()`.

```{r eval = FALSE, echo = TRUE}
mvbind(Reaction, Memory) ~ Days + (1 + Days | Subject)
```

. . .

The right-hand side is used to define the predictors. The intercept is is generally implicit, so that the two formulations below are equivalent.

```{r eval = FALSE, echo = TRUE}
mvbind(Reaction, Memory) ~ Days + (1 + Days | Subject)
mvbind(Reaction, Memory) ~ 1 + Days + (1 + Days | Subject)
```

## Syntax reminders

If you want to fit a model without an intercept (because why not), you must specify it explicitly, as shown below.

```{r eval = FALSE, echo = TRUE}
mvbind(Reaction, Memory) ~ 0 + Days + (1 + Days | Subject)
```

. . .

By default `brms` assumes a Gaussian likelihood function. This assumption can be modified easily by specifying the desired likelihood via the the `family` argument.

```{r eval = FALSE, echo = TRUE}
brm(Reaction ~ 1 + Days + (1 + Days | Subject), family = lognormal() )
```

. . .

In case of doubt, read the documentation (it's very exciting to read) available at
`?brm`.

## Some useful functions

```{r fonctions-utiles, eval = FALSE, echo = TRUE}
# retrieving the Stan code generated by brms
make_stancode(formula, ...)
stancode(fit)

# checking and/or defining priors
get_prior(formula, ...)
set_prior(prior, ...)

# retrieving predictions from a brms model
fitted(fit, ...)
predict(fit, ...)
conditional_effects(fit, ...)

# posterior predictive checking
pp_check(fit, ...)

# model comparison and hypothesis testing 
loo(fit1, fit2, ...)
bayes_factor(fit1, fit2, ...)
model_weights(fit1, fit2, ...)
hypothesis(fit, hypothesis, ...)
```

## A first example

```{r mod1, eval = TRUE, echo = TRUE, results = "hide"}
library(brms)
mod1 <- brm(height ~ 1, data = d2)
```

. . .

```{r summary-mod1, eval = TRUE, echo = TRUE}
posterior_summary(mod1, pars = c("^b_", "sigma"), probs = c(0.025, 0.975) )
```

These data represent the marginal (posterior) distributions of each parameter. In other words, the posterior probability $\mu$, averaged over all possible values of $\sigma$, is best described by a Gaussian distribution with a mean of $154.6$ and a standard deviation of $0.42$. The credibility interval ($\neq$ confidence interval) indicates the 95% most plausible values of $\mu$ or $\sigma$ (given the data and priors).

## Using our prior

By default `brms` uses a very uninformative prior centred on the mean value of the measured variable. We can therefore refine the estimate using our knowledge of the usual distribution of heights in humans.

. . .

The `get_prior()` function is used to display a list of default priors as well as all the priors we can specify, given a certain model formula (i.e., a way of writing our model) and a set of data.

```{r get-prior, eval = TRUE, echo = TRUE}
get_prior(height ~ 1, data = d2)
```

## Using our prior

```{r mod2, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(174, 20), class = Intercept),
  prior(exponential(0.01), class = sigma)
  )

mod2 <- brm(
  height ~ 1,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

```{r prior-mod2, echo = FALSE, fig.width = 15, fig.height = 5}
library(patchwork)

p1 <- data.frame(x = c(100, 250) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dnorm, args = list(mean = 174, sd = 20),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  labs(x = expression(mu), y = "Probability density")

p2 <- data.frame(x = c(0, 500) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dexp, args = list(0.01),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  labs(x = expression(sigma), y = "Probability density")

p1 + p2
```

## Using our prior

```{r summary-mod2, eval = TRUE, echo = TRUE}
summary(mod2)
```

## Using a more informative prior

```{r mod3, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(174, 0.1), class = Intercept),
  prior(exponential(0.01), class = sigma)
  )

mod3 <- brm(
  height ~ 1,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

```{r prior-mod3, echo = FALSE, fig.width = 15, fig.height = 5}
library(patchwork)

p1 <- data.frame(x = c(173, 175) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dnorm, args = list(mean = 174, sd = 0.1),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  labs(x = expression(mu), y = "Probability density")

p2 <- data.frame(x = c(0, 500) ) %>%
  ggplot(aes(x = x) ) +
  stat_function(
    fun = dexp, args = list(0.01),
    fill = "steelblue", geom = "area", alpha = 0.8
    ) +
  labs(x = expression(sigma), y = "Probability density")

p1 + p2
```

## Using a more informative prior

```{r summary-mod3, eval = TRUE, echo = TRUE}
summary(mod3)
```

We note that the estimated value for $\mu$ has hardly "moved" from the prior...but we can also see that the estimated value for $\sigma$ has greatly increased. What happened? We told the model that we were fairly certain of our $\mu$ value, the model then "adapted", which explains the large value of $\sigma$...

## Prior precision (heuristic)

Prior distributions can generally be considered as posterior distributions obtained from previous data.

. . .

We know that the $\sigma$ of a Gaussian posterior is given by:

$$\sigma_{\text{post}} = 1\ /\ \sqrt{n}$$

Which implies a **quantity of data** $n = 1\ /\sigma^2_{\text{post}}$. Our prior had a $\sigma = 0.1$, which implies $n = 1\ /\ 0.1^2 = 100$.

. . .

We can therefore consider that the prior $\mathrm{Normal}(174, 0.1)$ is equivalent to the case in which we would have observed $100$ heights with mean $174$.

## Visualising samples from the posterior distribution

```{r get-density-function, eval = TRUE, echo = FALSE}
library(viridis)
library(MASS)

# Get density of points in 2 dimensions.
# @param x A numeric vector.
# @param y A numeric vector.
# @param n Create a square n by n grid to compute density.
# @return The density within each square.

get_density <- function(x, y, n = 100) {
    
    dens <- MASS::kde2d(x = x, y = y, n = n)
    ix <- findInterval(x, dens$x)
    iy <- findInterval(y, dens$y)
    ii <- cbind(ix, iy)
    return(dens$z[ii])
    
}
```

```{r samples-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
post <- as_draws_df(x = mod2) %>%
    mutate(density = get_density(b_Intercept, sigma, n = 1e2) )

ggplot(post, aes(x = b_Intercept, y = sigma, color = density) ) +
    geom_point(size = 2, alpha = 0.5, show.legend = FALSE) +
    labs(x = expression(mu), y = expression(sigma) ) +
    viridis::scale_color_viridis()
```

## Retrieving samples from the posterior distribution

```{r eval = TRUE, echo = TRUE}
# gets the first 6 samples
head(post)
```

. . .

```{r eval = TRUE, echo = TRUE}
# gets the median and the 95% credible interval
t(sapply(post[, 1:2], quantile, probs = c(0.025, 0.5, 0.975) ) )
```

## Visualising the posterior distribution

```{r eval = FALSE, echo = TRUE}
H.scv <- Hscv(post[, 1:2])
fhat_post <- kde(x = post[, 1:2], H = H.scv, compute.cont = TRUE)

plot(fhat_post, display = "persp", col = "purple", border = NA,
  xlab = "\nmu", ylab = "\nsigma", zlab = "\np(mu, sigma)",
  shade = 0.8, phi = 30, ticktype = "detailed",
  cex.lab = 1.2, family = "Helvetica")
```

```{r posterior-plot, echo = FALSE, out.width = "600px"}
knitr::include_graphics("figures/posterior.png")
```

## Visualising the posterior distribution

```{r plot-samples, eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 12, fig.height = 8}
library(viridis)

sample.mu <- post$b_Intercept
sample.sigma <- post$sigma

data.frame(sample.mu, sample.sigma) %>%
    ggplot(aes(x = sample.mu, y = sample.sigma) ) + 
    stat_density_2d(
        geom = "raster",
        aes(fill = ..density..),
        contour = FALSE, show.legend = FALSE
        ) +
    geom_vline(xintercept = mean(sample.mu), lty = 2) +
    geom_hline(yintercept = mean(sample.sigma), lty = 2) +
    scale_fill_viridis(na.value = "black") +
    coord_cartesian(
        xlim = c(min(sample.mu), max(sample.mu) ),
        ylim = c(min(sample.sigma), max(sample.sigma) )
        ) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) +
    labs(x = expression(mu), y = expression(sigma) )
```

## Including predictors

How does height change with weight?

```{r height-weight-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
d2 %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8)
```

## Linear regresion with a single continuous predictor

$$
\begin{align}
h_{i} &\sim \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta x_{i} \\
\end{align}
$$

```{r lm-regression, eval = TRUE, echo = TRUE, fig.align = "center"}
linear_model <- lm(height ~ weight, data = d2)
rethinking::precis(linear_model, prob = 0.95)
```

```{r lm-regression-plot, eval = TRUE, echo = FALSE, fig.align = "center", fig.width = 7.5, fig.height = 5}
d2 %>%
    ggplot(aes(x = weight, y = height) ) +
    geom_point(
      colour = "white", fill = "black",
      pch = 21, size = 3, alpha = 0.8
      ) +
    geom_smooth(method = "lm", se = FALSE, color = "black", lwd = 1)
```

## Notations

Let's consider a linear regression model with a single predictor, a slope, an intercept, and residuals distributed according to a normal distribution. The notation:

$$
h_{i} = \alpha + \beta x_{i} + \epsilon_{i} \quad \text{with} \quad \epsilon_{i} \sim \mathrm{Normal}(0, \sigma)
$$

is equivalent to:

$$
h_{i} - (\alpha + \beta x_{i}) \sim \mathrm{Normal}(0, \sigma)
$$

. . .

rearranging the expression:

$$
h_{i} \sim \mathrm{Normal}(\alpha + \beta x_{i}, \sigma).
$$

The above notations are equivalent, but the last one is more flexible, and will allow us to extend it more intuitively to multilevel models.

## Linear regresion with a single continuous predictor

$$
\begin{aligned}
\color{orangered}{h_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i},\sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha + \beta x_{i}} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal}(174, 20)} \\
\color{steelblue}{\beta} \ &\color{steelblue}{\sim \mathrm{Normal}(0, 10)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{Exponential}(0.01)} \\
\end{aligned}
$$

In this model, $\mu$ is no longer a parameter to be estimated (because $\mu$ is **determined** by $\alpha$ and $\beta$). Instead, we will estimate $\alpha$ and $\beta$.

. . .

Reminders: $\alpha$ is the intercept, that is, the expected height, when the weight is equal to $0$. $\beta$ is the slope, that is, the expected change in height when weight increases by one unit.

## Linear regresion with a single continuous predictor

```{r mod4, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(174, 20), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(exponential(0.01), class = sigma)
  )

mod4 <- brm(
  height ~ 1 + weight,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

## Linear regresion with a single continuous predictor

```{r summary-mod4, eval = TRUE, echo = TRUE}
posterior_summary(mod4)
```

-   $\alpha = 113.90, 95\% \ \text{CrI} \ [110.28, 117.59]$ represents the expected height when weight equals 0kg...

-   $\beta = 0.90, 95\% \ \text{CrI} \ [0.82, 0.98]$ indicates that for an increase of 1kg, we can expect an increase of 0.90cm.

## Linear regresion with a single continuous predictor

```{r mod5, eval = TRUE, echo = TRUE, results = "hide"}
d2$weight.c <- d2$weight - mean(d2$weight)

mod5 <- brm(
  height ~ 1 + weight.c,
  prior = priors,
  family = gaussian(),
  data = d2
  )
```

. . .

```{r fixef-mod5, eval = TRUE, echo = TRUE}
fixef(mod5) # retrieving the fixed effects estimates
```

If the predictor (weight) is centred, the intercept represents the expected value of height (in cm) when weight is at its mean value.

## Visualising predictions from the model

```{r mod4-predictions, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
d2 %>%
    ggplot(aes(x = weight, y = height) ) +
    geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
    geom_abline(intercept = fixef(mod4)[1], slope = fixef(mod4)[2], lwd = 1)
```

## Visualising the uncertainty on $\mu$ using fitted()

```{r fitted-mod4, eval = TRUE, echo = TRUE}
# defining a grid of possible values for "weight"
weight.seq <- data.frame(weight = seq(from = 25, to = 70, by = 1) )

# retrieving the model's predicted mus (average height) for these values of "weight"
mu <- data.frame(fitted(mod4, newdata = weight.seq) ) %>% bind_cols(weight.seq)

# displaying the first 10 rows of mu
head(mu, 10)
```

## Visualising the uncertainty on $\mu$ using fitted()

```{r fitted-mod4-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 5}
d2 %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
    stat = "identity",
    color = "black", alpha = 0.8, size = 1
    )
```

## Prediction intervals (incorporating $\sigma$)

As a reminder, our model is defined as: $h_{i} \sim \mathrm{Normal}(\alpha + \beta x_{i}, \sigma)$. For the moment, we have only represented the predictions for $\mu$. How can we incorporate $\sigma$ into our predictions?

```{r predict-mod4, eval = TRUE, echo = TRUE}
# defining a grid of possible values for "weight"
weight.seq <- data.frame(weight = seq(from = 25, to = 70, by = 1) )

# retrieving the model's predicted heights for these values of "weight"
pred_height <- data.frame(predict(mod4, newdata = weight.seq) ) %>% bind_cols(weight.seq)

# displaying the first 10 rows of pred_height
head(pred_height, 10)
```

## Prediction intervals (incorporating $\sigma$)

```{r predict-mod4-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 4}
d2 %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_ribbon(
    data = pred_height, aes(x = weight, ymin = Q2.5, ymax = Q97.5),
    alpha = 0.2, inherit.aes = FALSE
    ) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
    stat = "identity", color = "black", alpha = 0.8, size = 1
    )
```

## Built-in brms functions

The `brms` package also includes the `posterior_epred()`, `posterior_linpred()`, and `posterior_predict()` functions, which can be used to generate predictions from models fitted with `brms`. Andrew Heiss gives a detailed description of how these functions work in [this blog post](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/).

![](figures/normal.png){fig-align="center"}

## Reminder: Two types of uncertainty

Two sources of uncertainty in the model: uncertainty concerning the estimation of the value of the parameters but also uncertainty concerning the sampling process.

. . .

**Epistemic uncertainty**: The posterior distribution orders all possible combinations of parameter values according to their relative plausibility.

. . .

**Aleatory uncertainty**: The distribution of simulated data is a distribution that contains uncertainty related to a sampling process (i.e., generating data from a Gaussian distribution).

. . .

See also this short article by @ohagan2004.

## Polynomial regression

```{r plot-poly, eval = TRUE, echo = TRUE, fig.align = "center"}
d %>% # note that we use d instead of d2
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8)
```

## Standardised (scaled) scores

```{r poly-plot-std, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 6, fig.height = 4}
d <- d %>% mutate(weight.s = (weight - mean(weight) ) / sd(weight) )

d %>%
    ggplot(aes(x = weight.s, y = height) ) +
    geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8)

c(mean(d$weight.s), sd(d$weight.s) )
```

## Standardised (scaled) scores

Why standardising predictors?

- **Interpretation**. Easier to compare the coefficients of several predictors. A change of one standard deviation in the predictor corresponds to to a change of one standard deviation in the response (if the response is also standardised).

- **Fitting**. When the predictors contain large values (or values that are too different from one another), this can lead to convergence problems (cf. Course n°03).

## Polynomial regression - Exercise

$$
\begin{aligned}
\color{orangered}{h_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha + \beta_{1} x_{i} + \beta_{2} x_{i}^{2}} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal}(156, 100)} \\
\color{steelblue}{\beta_{1}, \beta_{2}} \ &\color{steelblue}{\sim \mathrm{Normal}(0, 10)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{Exponential}(0.01)} \\
\end{aligned}
$$

It's up to you to build and fit this model using `brms::brm()`.

## Polynomial regression

```{r mod6, eval = TRUE, echo = TRUE, results = "hide"}
priors <- c(
  prior(normal(156, 100), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(exponential(0.01), class = sigma)
  )

mod6 <- brm(
  # NB: polynomials should be written with the I() function...
  height ~ 1 + weight.s + I(weight.s^2),
  prior = priors,
  family = gaussian(),
  data = d
  )
```

## Polynomial regression

```{r summary-mod6, eval = TRUE, echo = TRUE}
summary(mod6)
```

## Visualising the model's predictions

```{r predict-mod6, eval = TRUE, echo = TRUE}
# defining a grid of possible (standardised) values for "weight"
weight.seq <- data.frame(weight.s = seq(from = -2.5, to = 2.5, length.out = 50) )

# retrieving the model's predictions for these values of weight
mu <- data.frame(fitted(mod6, newdata = weight.seq) ) %>% bind_cols(weight.seq)
pred_height <- data.frame(predict(mod6, newdata = weight.seq) ) %>% bind_cols(weight.seq)

# displaying the first ten rows of pred_height
head(pred_height, 10)
```

## Visualising the model's predictions

```{r predict-mod6-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 5}
d %>%
  ggplot(aes(x = weight.s, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_ribbon(
    data = pred_height, aes(x = weight.s, ymin = Q2.5, ymax = Q97.5),
    alpha = 0.2, inherit.aes = FALSE
    ) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
    stat = "identity", color = "black", alpha = 0.8, size = 1
    )
```

## Effect sizes in regression models

There are several methods for computing effect sizes in Bayesian models. @gelman2006 propose a method for computing a sample-based $R^{2}$.

. . .

@marsman2017 and @marsman2019 generalise existing methods to compute a $\rho^{2}$ for ANOVA designs (i.e., with categorical predictors), which represents an estimate of the effect size in the population.

. . .

> Similar to most of the ES measures that have been proposed for the
> ANOVA model, the squared multiple correlation coefficient $\rho^{2}$
> \[...\] is a so-called proportional reduction in error measure (PRE).
> In general, a PRE measure expresses the proportion of the variance in
> an outcome $y$ that is attributed to the independent variables $x$
> [@marsman2019].

## Effect sizes in regression models

$$
\begin{aligned}
\rho^{2} &= \dfrac{\sum_{i = 1}^{n} \pi_{i}(\beta_{i} - \beta)^{2}}{\sigma^{2} + \sum_{i=1}^{n} \pi_{i}(\beta_{i} - \beta)^{2}} \\  \rho^{2} &= \dfrac{ \frac{1}{n} \sum_{i=1}^{n} \beta_{i}^{2}}{\sigma^{2} + \frac{1}{n} \sum_{i = 1}^{n} \beta_{i}^{2}} \\ \rho^{2} &= \dfrac{\beta^{2} \tau^{2}}{\sigma^{2} + \beta^{2} \tau^{2}}\\
\end{aligned}
$$

```{r effsize, eval = TRUE, echo = TRUE}
post <- as_draws_df(x = mod4)
beta <- post$b_weight
sigma <- post$sigma
rho <- beta^2 * var(d2$weight) / (sigma^2 + beta^2 * var(d2$weight) )
```

Caution: if there are several predictors, it depends on the covariance structure...

## Effect sizes in regression models

```{r effsize-plot1, eval = TRUE, echo = TRUE, dev = "png", dpi = 200}
posterior_plot(samples = rho, usemode = TRUE) + labs(x = expression(rho) )
```

```{r summary-lm-effsize, eval = TRUE, echo = TRUE}
summary(lm(height ~ weight, data = d2) )$r.squared
```

## Effect sizes in regression models

```{r effsize-plot2, eval = TRUE, echo = TRUE, dev = "png", dpi = 200}
bayes_R2(mod4)
```

. . .

```{r effsize-plot3, eval = TRUE, echo = TRUE, dev = "png", dpi = 200}
bayes_R2(mod4, summary = FALSE)[, 1] %>%
    posterior_plot(usemode = TRUE) +
    labs(x = expression(rho) )
```

## Summary

A new model with two and then three parameters was presented: a Gaussian model, then a Gaussian linear regression model, used to relate two continuous variables.

. . .

As previously, Bayes' theorem is used to update our prior knowledge of parameter values into posterior knowledge, a synthesis between our priors and the information contained in the data.

. . .

The `brms` package can be used to fit all sorts of models with a syntax similar to that used by `lm()`.

. . .

The `fitted()` function is used to retrieve the predictions of a model.

. . .

The `predict()` function is used to simulate data from a model fitted with `brms`.

## Practical work - 1/2

Select all rows in the `howell` dataset corresponding to minors individuals (age < 18). This should result in a dataframe of 192 rows.

. . .

Fit a linear regression model using the `brms::brm()` function. Report and interpret the estimates from this model. For an increase of 10 units in `weight`, what increase in height (`height`) does the model predict?

. . .

Plot the raw data with weight on the x-axis and height on the y-axis. Overlay the model's regression line of the model and an 89% credibility interval for the mean. Add an 89% credibility interval for the predicted sizes.

. . .

What do you think of the 'fit' of the model? What assumptions of the model would you be willing to modify in order to improve the model's fit?

## Practical work - 2/2

Let's say you've consulted a colleague who's an expert in [allometry](https://fr.wikipedia.org/wiki/Allométrie) (i.e., the
phenomena of differential organ growth) and she explains to you that it doesn't make sense to model the relationship between weight and height... because we know that it's the logarithm of weight which is (linearly) related to height!

. . .

Model the relationship between height (cm) and log weight (log-kg). Use the entire `howell` dataframe (all 544 rows). Fit the following model using `brms::brm()`.

$$
\begin{align*}
&\color{orangered}{h_{i} \sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
&\mu_{i}= \alpha + \beta \cdot \log (w_{i}) \\
&\color{steelblue}{\alpha \sim \mathrm{Normal}(174, 100)} \\
&\color{steelblue}{\beta \sim \mathrm{Normal}(0, 100)} \\
&\color{steelblue}{\sigma \sim \mathrm{Exponential}(0.01)} \\
\end{align*}
$$

Where $h_{i}$ is the height of individual $i$ and $w_{i}$ the weight of individual $i$. The function for calculating the log in `R` is simply `log()`. Can you interpret the results? Hint: plot the raw data and superimpose the model's predictions...

## Proposed solution

```{r mod7, eval = TRUE, echo = TRUE, results = "hide"}
# we keep only the individuals younger than 18
d <- open_data(howell) %>% filter(age < 18)

priors <- c(
  prior(normal(150, 100), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(exponential(0.01), class = sigma)
  )

mod7 <- brm(
  height ~ 1 + weight,
  prior = priors,
  family = gaussian(),
  data = d
  )
```

## Proposed solution

```{r summary-mod7, eval = TRUE, echo = TRUE}
summary(mod7, prob = 0.89)
```

## Proposed solution

```{r predict-mod7, eval = TRUE, echo = TRUE}
# defining a grid of possible values for "weight"
weight.seq <- data.frame(weight = seq(from = 5, to = 45, length.out = 1e2) )

# retrieving the model's predictions for these values fo "weight"
mu <- data.frame(
  fitted(mod7, newdata = weight.seq, probs = c(0.055, 0.945) )
  ) %>%
  bind_cols(weight.seq)

pred_height <- data.frame(
  predict(mod7, newdata = weight.seq, probs = c(0.055, 0.945) )
  ) %>%
  bind_cols(weight.seq)

# displaying the first 6 rows of pred_height
head(pred_height)
```

## Proposed solution

```{r predict-mod7-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 5}
d %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_ribbon(
    data = pred_height, aes(x = weight, ymin = Q5.5, ymax = Q94.5),
    alpha = 0.2, inherit.aes = FALSE
    ) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
    stat = "identity", color = "black", alpha = 0.8, size = 1
    )
```

## Proposed solution

```{r mod8, eval = TRUE, echo = TRUE, results = "hide"}
# we now consider all individuals
d <- open_data(howell)

mod8 <- brm(
  # we now predict height using the logarithm of weight
  height ~ 1 + log(weight),
  prior = priors,
  family = gaussian(),
  data = d
  )
```

## Proposed solution

```{r summary-mod8, eval = TRUE, echo = TRUE}
summary(mod8, prob = 0.89)
```

## Proposed solution

```{r predict-mod8, eval = TRUE, echo = TRUE}
# defining a grid of possible values for "weight"
weight.seq <- data.frame(weight = seq(from = 5, to = 65, length.out = 1e2) )

# retrieving the model's predictions for these values of "weight"
mu <- data.frame(
  fitted(mod8, newdata = weight.seq, probs = c(0.055, 0.945) )
  ) %>%
  bind_cols(weight.seq)

pred_height <- data.frame(
  predict(mod8, newdata = weight.seq, probs = c(0.055, 0.945) )
  ) %>%
  bind_cols(weight.seq)

# displaying the first 6 rows of "pred_height"
head(pred_height)
```

## Proposed solution

```{r predict-mod8-plot, eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 5}
d %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(colour = "white", fill = "black", pch = 21, size = 3, alpha = 0.8) +
  geom_ribbon(
    data = pred_height, aes(x = weight, ymin = Q5.5, ymax = Q94.5),
    alpha = 0.2, inherit.aes = FALSE
    ) +
  geom_smooth(
    data = mu, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
    stat = "identity", color = "black", alpha = 0.8, size = 1
    )
```

## References {.refs}
