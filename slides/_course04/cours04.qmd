---
title: Introduction to Bayesian statistical modelling
subtitle: A course with R, Stan, and brms
author: Ladislas Nalborczyk (UNICOG, NeuroSpin, CEA, Gif/Yvette, France)
from: markdown+emoji
format:
  revealjs:
    incremental: true
    theme: [default, ../custom.scss]
    transition: none # fade
    background-transition: none # fade
    transition-speed: default # default, fast, or slow
    slide-number: c/t
    show-slide-number: all
    preview-links: true
    self-contained: true # when sharing slides
    # chalkboard: true
    csl: ../../files/bib/apa7.csl
    logo: ../../files/cover.png
    footer: "Ladislas Nalborczyk - IBSM2023"
    # width: 1200 # defaults to 1050
    # height: 900 # default to 700
    margin: 0.15 # defaults to 0.1
    scrollable: true
    hide-inactive-cursor: true
    pdf-separate-fragments: false
    highlight-style: zenburn
    code-copy: true
    code-link: false
    code-fold: false
    code-summary: "See the code"
    numbers: true
    progress: false
title-slide-attributes:
    data-background-color: "#1c5253"
bibliography: ../../files/bib/references.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, eval = TRUE, include = FALSE, cache = FALSE}
library(tidyverse)
library(brms)

# setting up knitr options
knitr::opts_chunk$set(
  cache = TRUE, echo = TRUE,
  warning = FALSE, message = FALSE,
  fig.align = "center", dev = "svg"
  )

# setting up ggplot theme
theme_set(theme_bw(base_size = 16, base_family = "Open Sans") )
```

## Planning

Course n°01: Introduction to Bayesian inference, Beta-Binomial model <br> Course n°02: Introduction to brms, linear regression <br> Course n°03: Markov Chain Monte Carlo, generalised linear model <br> **Course n°04: Multilevel models, cognitive models** <br>

$$\newcommand\given[1][]{\:#1\vert\:}$$

## Multilevel models

The aim is to build a model that can **learn at several levels**, that can produce estimates that will be informed by the different groups present in the data. We will follow the following example throughout this course.

. . .

Let's imagine that we've built a robot that visits cafés, and that has fun measuring the waiting time after ordering. This
robot visits 20 different cafés, 5 times in the morning and 5 times in the afternoon, and measures the time (in minutes) it takes to get a coffee.

```{r echo = FALSE, out.width = "300px"}
knitr::include_graphics("figures/robot.png")
```

## Coffee robot

```{r eval = TRUE, echo = TRUE}
library(tidyverse)
library(imsb)

df <- open_data(robot)
head(x = df, n = 15)
```

## Coffee robot

```{r eval = TRUE, echo = TRUE, fig.width = 15, fig.height = 5}
df %>%
  ggplot(aes(x = factor(cafe), y = wait, fill = factor(afternoon) ) ) +
  geom_dotplot(
    stackdir = "center", binaxis = "y",
    dotsize = 1, show.legend = FALSE
    ) +
  geom_hline(yintercept = mean(df$wait), linetype = 3) +
  facet_wrap(~afternoon, ncol = 2) +
  labs(x = "Café", y = "Waiting time (min)")
```

## Coffee robot, a first model

An initial model can be built, estimating the average time (across all cafés combined) to be served.

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal}(5, 10)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\end{align}
$$

## Half-Cauchy

$$
p(x \given x_{0}, \gamma) = \left(\pi \gamma \left[1 + \left(\frac{x-x_{0}}{\gamma}\right)^{2}\right] \right)^{-1}
$$

```{r eval = TRUE, echo = TRUE, ig.width = 7.5, fig.height = 5}
ggplot(data = data.frame(x = c(0, 10) ), aes(x = x) ) +
    stat_function(
        fun = dcauchy,
        args = list(location = 0, scale = 2), size = 1.5
        )
```

## Coffee robot, a first model

```{r eval = TRUE, echo = TRUE, results = "hide"}
library(brms)

mod1 <- brm(
  formula = wait ~ 1,
  prior = c(
    prior(normal(5, 10), class = Intercept),
    prior(cauchy(0, 2), class = sigma)
    ),
  data = df,
  cores = parallel::detectCores()
  )
```

. . .

```{r eval = TRUE, echo = TRUE, warning = FALSE}
posterior_summary(x = mod1, probs = c(0.025, 0.975), pars = c("^b_", "sigma") )
```

## Diagnostic plot

```{r eval = TRUE, echo = TRUE, fig.width = 14, fig.height = 7}
plot(
  x = mod1, combo = c("dens_overlay", "trace"),
  theme = theme_bw(base_size = 16, base_family = "Open Sans")
  )
```

## One intercept per café

Second model which estimates one intercept per café. Equivalent to constructing 20 dummy variables.

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha_{\text{café}[i]}} \\
\color{steelblue}{\alpha_{\text{café}[i]}} \ &\color{steelblue}{\sim \mathrm{Normal}(5, 10)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\end{align}
$$

```{r eval = TRUE, echo = TRUE, results = "hide"}
mod2 <- brm(
  formula = wait ~ 0 + factor(cafe),
  prior = c(
    prior(normal(5, 10), class = b),
    prior(cauchy(0, 2), class = sigma)
    ),
  data = df,
  cores = parallel::detectCores()
  )
```

## One intercept per café

```{r eval = TRUE, echo = TRUE, warning = FALSE}
posterior_summary(x = mod2, pars = "^b_")
```

## Multilevel model

Couldn't we ensure that the time measured at café 1 **informs** the measurement taken at café 2 and café 3? As well as the average time taken to be served? We're going to learn the priors from the data...

$$
\begin{align}
\text{Level 1}: \color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha_{\text{café}[i]}} \\
\text{Level 2}: \color{steelblue}{\alpha_{\text{café}}} \ &\color{steelblue}{\sim \mathrm{Normal}(\alpha,\sigma_{\text{café}})} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal}(5, 10)} \\
\color{steelblue}{\sigma_{\text{café}}} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\end{align}
$$

The prior for the intercept of each coffee ($\alpha_{\text{café}}$) is now a function of two parameters ($\alpha$ and
$\sigma_{\text{café}}$). $\alpha$ and $\sigma_{\text{café}}$ are called **hyper-parameters**, they are parameters for parameters, and their priors are called **hyperpriors**. There are two levels in the model...

## Equivalences

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha_{\text{café}[i]}} \\
\color{steelblue}{\alpha_{\text{café}}} \ &\color{steelblue}{\sim \mathrm{Normal}(\alpha,\sigma_{\text{café}})} \\
\end{align}
$$

NB: $\alpha$ is defined here in the prior for $\alpha_{\text{café}}$ but it could, in the same way, be defined in the linear model:

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha + \alpha_{\text{café}[i]}} \\
\color{steelblue}{\alpha_{\text{café}}} \ &\color{steelblue}{\sim \mathrm{Normal}(0,\sigma_{\text{café}})} \\
\end{align}
$$

We can always "remove" the mean from a Gaussian distribution and consider it as a constant plus a Gaussian centred on zero.

. . .

NB: when $\alpha$ is defined in the linear model, the $\alpha_{\text{café}}$ represent deviations from the mean intercept.
It is therefore necessary to add $\alpha$ and $\alpha_{\text{café}}$ to obtain the average waiting time per café...

## Equivalences

```{r eval = TRUE, echo = TRUE, out.width = "33%"}
y1 <- rnorm(n = 1e4, mean = 5, sd = 1)
y2 <- rnorm(n = 1e4, mean = 0, sd = 1) + 5

data.frame(y1 = y1, y2 = y2) %>%
    pivot_longer(cols = 1:2, names_to = "x", values_to = "y") %>%
    ggplot(aes(x = y, colour = x) ) +
    geom_density(show.legend = FALSE)
```

## {background-iframe="http://mfviz.com/hierarchical-models/"}

## Multilevel model

```{r eval = TRUE, echo = TRUE, results = "hide"}
mod3 <- brm(
  formula = wait ~ 1 + (1 | cafe),
  prior = c(
    prior(normal(5, 10), class = Intercept),
    prior(cauchy(0, 2), class = sigma),
    prior(cauchy(0, 2), class = sd)
    ),
  data = df,
  warmup = 1000, iter = 5000,
  cores = parallel::detectCores()
  )
```

This model has 23 parameters, the general intercept $\alpha$, the residual variability $\sigma$, the variability between cafés and one intercept per café.

## Shrinkage

```{r echo = FALSE, fig.width = 14, fig.height = 8}
library(wesanderson) # for plotting
post <- as_draws_df(mod3) # retrieving posterior samples

df %>%
    group_by(cafe) %>%
    summarise(Observed = mean(wait) ) %>%
    mutate(Estimated = coef(mod3)$cafe[, , ] %>% data.frame %>% pull(Estimate) ) %>%
    gather(type, Observed, Observed:Estimated) %>%
    ggplot(aes(x = cafe, y = Observed, fill = type) ) +
    geom_hline(yintercept = mean(post$b_Intercept), linetype = 2) +
    geom_point(pch = 21, size = 5, alpha = 0.8, colour = "white", show.legend = TRUE) +
    scale_color_manual(values = rev(wes_palette(n = 2, name = "Chevalier1") ) )  +
    scale_fill_manual(values = rev(wes_palette(n = 2, name = "Chevalier1") ) )  +
    scale_x_continuous(name = "Café", breaks = 1:20) +
    ylab("Waiting time (min)") +
    theme(legend.title = element_blank() )
```

## Shrinkage magic [@efron1977]

```{r echo = FALSE, fig.align = "center", out.width = "66%"}
knitr::include_graphics("figures/stein1.png")
```

The James-Stein estimator is defined as $z = \bar{y} + c(y - \bar{y})$,
where $bar{y}$ is the sample mean, $y$ is an individual observation,
and $c$ is a constant, the **shrinking factor** [@efron1977].

## Shrinkage magic [@efron1977]

The shrinking factor is determined both by the variability (imprecision) of the measurement (e.g., its standard deviation) and by the distance to the mean estimate (i.e., $y - \bar{y}$). In other words, this estimator is less "confident" about (i.e., gives less weight to) imprecise and/or extreme observations. In practice, shrinkage acts as a safeguard against overlearning (overfitting).

```{r echo = FALSE, fig.align = "center", out.width = "75%"}
knitr::include_graphics("figures/stein2.png")
```

## Pooling

The **shrinkage** observed on the previous slide is due to information pooling between cafés. The estimate of the
intercept for each café informs the intercept estimates of the other cafés, as well as the estimate of the general intercept (i.e., the overall average waiting time).

. . .

There are generally three perspectives (or strategies):

- **Complete pooling**: the waiting time is assumed to be invariant, a common intercept (`mod1`) is estimated.

- **No pooling**: it is assumed that each café's waiting time is unique and independent: an intercept is estimated for each café, but without informing the higher level (`mod2`).

- **Partial pooling**: an adaptive prior is used, as in the previous example (`mod3`).

. . .

The **complete pooling** strategy generally underfits the data (low predictive capacity) whereas the **no pooling** strategy amounts to overfitting the data (low predictive capacity here too). The **partial pooling** strategy (i.e., that of multilevel models) balances underfitting and overfitting.

## Model comparison

We can compare these models using indices derived from information theory (extensions of AIC), such as the WAIC (the lower the better).

```{r eval = TRUE, echo = TRUE}
# computing the WAIC for each model and storing it
mod1 <- add_criterion(mod1, "waic")
mod2 <- add_criterion(mod2, "waic")
mod3 <- add_criterion(mod3, "waic")

# comparing these WAICs
w <- loo_compare(mod1, mod2, mod3, criterion = "waic")
print(w, simplify = FALSE)
```

We note that model 3 has only 18 effective parameters (pWAIC) and fewer parameters than model 2, whereas it actually has
2 more... `posterior_summary(mod3)[3, 1]` gives us the sigma of the adaptive priority of $\alpha_{\text{café}}$ ($\sigma_{\text{café}} = 0.82$). Note that this sigma is very low is very low and corresponds to assigning a very restrictive or **regularising** prior.

## Model comparaison

The estimates of the first model (complete pooling model) and the third model (partial pooling model) are compared.

```{r eval = TRUE, echo = TRUE}
posterior_summary(mod1, pars = c("^b", "sigma") )
posterior_summary(mod3, pars = c("^b", "sigma") )
```

Both models make the same prediction (on average) for $\alpha$, but model 3 is more uncertain of its prediction than model 1 (see the standard error for $\alpha$)...

. . .

The $\sigma$ estimate of model 3 is smaller than that of model 1 because model 3 **decomposes** the unexplained variability into two sources: variability in waiting time between cafés and the residual variability $\sigma$.

## Coffee robot

Let's assume that our robot doesn't visit all the cafés the same number of times (as in the previous case) but that it visits more often the cafés close to home...

```{r eval = TRUE, echo = TRUE, results = "hide"}
df2 <- open_data(robot_unequal) # new dataset

mod4 <- brm(
  formula = wait ~ 1 + (1 | cafe),
  prior = c(
    prior(normal(5, 10), class = Intercept),
    prior(cauchy(0, 2), class = sigma),
    prior(cauchy(0, 2), class = sd)
    ),
  data = df2,
  warmup = 1000, iter = 5000,
  cores = parallel::detectCores()
  )
```

## Shrinkage

We can see that cafés that are visited frequently (right) are less affected by the effect of **shrinkage**. Their estimates are less "pulled" towards the average than the estimates of the least frequently visited cafés (left).

```{r echo = FALSE, fig.width = 12, fig.height = 6}
post <- as_draws_df(mod4)

df2 %>%
    group_by(cafe) %>%
    summarise(Observed = mean(wait) ) %>%
    mutate(Estimated = coef(mod4)$cafe[, , ] %>% data.frame %>% pull(Estimate) ) %>%
    gather(type, Observed, Observed:Estimated) %>%
    ggplot(aes(x = cafe, y = Observed, fill = type) ) +
    geom_hline(yintercept = mean(post$b_Intercept), linetype = 2) +
    geom_point(pch = 21, size = 5, alpha = 0.8, colour = "white", show.legend = TRUE) +
    scale_color_manual(values = rev(wes_palette(n = 2, name = "Chevalier1") ) )  +
    scale_fill_manual(values = rev(wes_palette(n = 2, name = "Chevalier1") ) )  +
    scale_x_continuous(name = "Café (from the least to the most visited)", breaks = 1:20) +
    ylab("Waiting time (min)") +
    theme(legend.title = element_blank() )
```

## Aparté: fixed and random effects

Five (contradictory) definitions identified by @gelman2005.

-   Fixed effects are constant across individuals, and random effects
    vary.
-   Effects are fixed if they are interesting in themselves or random if
    there is interest in the underlying population.
-   When a sample exhausts the population, the corresponding variable is
    fixed; when the sample is a small (i.e., negligible) part of the
    population the corresponding variable is random.
-   If an effect is assumed to be a realized value of a random variable,
    it is called a random effect.
-   Fixed effects are estimated using least squares (or, more generally,
    maximum likelihood) and random effects are estimated with shrinkage.

. . .

@gelman2006a suggest instead the use of the terms **constant effcts** and **varying effects**, and to always use multilevel modelling, considering that the so-called **fixed effect** can simply be considered as a **random effect** whose variance would be equal to $0$.

## Regularisation and terminology

Varying the intercepts for each café is simply another way of (adaptively) regularising, that is, reducing the weight
given to the data in the estimation. The model becomes able to estimate the extent to which the groups (in this case the cafés) are different, while estimating the characteristics of each café...

. . .

Difference between **cross-classified** (or "crossed") multilevel models and **nested or hierarchical** multilevel models. Cross-classified models refer to data structured according to two (or more) non-nested random factors
Hierarchical models usually refers to hierarchically structured data (e.g., a student in a class in a school in a city...). See [this discussion](https://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified) for more details.

. . .

However, the two types of models are written in a similar way, on several "levels". The term "multilevel" (in our terminology) therefore refers to the structure of the model, to its specification. It is distinct from the structure of the data.

<!--

## Example of "cross-classified" model

We could ask ourselves whether the recency of the cafés (their age) might not be a source of uncontrolled variability?
All we have to do is add an intercept that varies by age, and assign it an adaptive prior.

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha + \alpha_{\text{café}[i]} + \alpha_{\text{age}[i]}} \\
\color{steelblue}{\alpha_{\text{café}}} \ &\color{steelblue}{\sim \mathrm{Normal}(5, \sigma_{\text{café}})} \\
\color{steelblue}{\alpha_{\text{âge}}} \ &\color{steelblue}{\sim \mathrm{Normal}(5, \sigma_{\text{age}})} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal}(0, 10)} \\
\color{steelblue}{\sigma_{\text{café}}} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\color{steelblue}{\sigma_{\text{âge}}} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{HalfCauchy}(0, 2)} \\
\end{align}
$$

-->

## Coffee robot: varying intercept + varying slope

We are now interested in the effect of the time of day on the waiting time. Do we wait more in the morning, or in the afternoon?

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha_{\text{café}[i]} + \beta_{\text{café}[i]} \times A_{i}} \\
\end{align}
$$

Where $A_{i}$ is a dummy variable coded 0/1 for morning/afternoon and where $\beta_{\text{café}}$ is therefore a difference parameter (i.e., a slope) between morning and afternoon.

. . .

Note: we know that cafés have intercepts and slopes which co-vary... Popular cafés will be overcrowded in the morning and much less in the afternoon, resulting in a significant slope. These cafés will also have a longer average waiting time (i.e., a larger intercept). In these cafés, $\alpha$ is large and $\beta$ is far from zero. Conversely, in an unpopular café, the waiting time will be short, as well as the difference between the morning and afternoon's waiting the.

. . .

We could therefore use the co-variation between the intercept and slope to make better inferences. In other words, ensure that the estimate of the intercept informs the estimate of the slope, and reciprocally.

## Coffee robot: varying intercept + varying slope

We are now interested in the effect of the time of day on the waiting time. Do we wait more in the morning, or in the afternoon?

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha_{\text{café}[i]} + \beta_{\text{café}[i]} \times A_{i}} \\
\color{steelblue}{\begin{bmatrix}
\alpha_{\text{café}} \\
\beta_{\text{café}} \\
\end{bmatrix}} \ &\color{steelblue}{\sim \mathrm{MVNormal}\bigg(\begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}\bigg)} \\
\end{align}
$$

The third line posits that every café has an intercept $\alpha_{\text{café}}$ and a slope $\beta_{\text{café}}$, defined by a bivariate (i.e., two-dimensional) Gaussian prior having as means $\alpha$ and $\beta$ and as covariance matrix $\textbf{S}$.

## Aparté: multivariate Normal distribution

$$\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$$

Where $\boldsymbol{\mu}$ is a ($k$-dimensional) vector of means, for instance: `mu <- c(a, b)`.

. . .

$\boldsymbol{\Sigma}$ is a covariance matrix of $k \times k$ dimensions, and which corresponds to the matrix given by the function `vcov()`.

$$
\begin{align}
\boldsymbol{\Sigma} &=
\begin{pmatrix}
\sigma_{\alpha}^{2} & \sigma_{\alpha} \sigma_{\beta} \rho \\
\sigma_{\alpha} \sigma_{\beta} \rho & \sigma_{\beta}^{2} \\
\end{pmatrix} \\
\end{align}
$$

## Aparté: multivariate Normal distribution

<!--

<center>
<iframe width=1200 height = 800 scrolling="no" frameborder="0" src="https://www.wolframcloud.com/obj/demonstrations/Published/TheBivariateNormalDistribution?_view=EMBED" style="border:0;"></iframe>
</center>

-->

```{r echo = FALSE, out.width = "800px"}
knitr::include_graphics("figures/bivariate.png")
```

## Aparté: multivariate Normal distribution

$$
\begin{align}
\boldsymbol{\Sigma} &=
\begin{pmatrix}
\sigma_{\alpha}^{2} & \sigma_{\alpha} \sigma_{\beta} \rho \\
\sigma_{\alpha} \sigma_{\beta} \rho & \sigma_{\beta}^{2} \\
\end{pmatrix} \\
\end{align}
$$

This matrix can be constructed in two different ways, strictly equivalent.

```{r eval = TRUE, echo = TRUE}
sigma_a <- 1
sigma_b <- 0.75
rho <- 0.7
cov_ab <- sigma_a * sigma_b * rho
(Sigma1 <- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2) )
```

## Aparté: multivariate Normal distribution

$$
\begin{align}
\boldsymbol{\Sigma} &=
\begin{pmatrix}
\sigma_{\alpha}^{2} & \sigma_{\alpha} \sigma_{\beta} \rho \\
\sigma_{\alpha} \sigma_{\beta} \rho & \sigma_{\beta}^{2} \\
\end{pmatrix} \\
\end{align}
$$

The second method is convenient because it considers separately the standard deviations and correlations.

```{r eval = TRUE, echo = TRUE}
(sigmas <- c(sigma_a, sigma_b) ) # standard deviations
(Rho <- matrix(c(1, rho, rho, 1), nrow = 2) ) # correlation matrix
(Sigma2 <- diag(sigmas) %*% Rho %*% diag(sigmas) )
```

## Coffee robot: varying intercept + varying slope

$$
\begin{align}
\color{orangered}{w_{i}} \ &\color{orangered}{\sim \mathrm{Normal}(\mu_{i}, \sigma)} \\
\color{black}{\mu_{i}} \ &\color{black}{= \alpha_{\text{café}[i]} + \beta_{\text{café}[i]} \times A_{i}} \\
\color{steelblue}{\begin{bmatrix}
\alpha_{\text{café}} \\
\beta_{\text{café}} \\
\end{bmatrix}} \ &\color{steelblue}{\sim \mathrm{MVNormal}\bigg(\begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}\bigg)} \\
\color{black}{\textbf{S}} \ &\color{black}{=
\begin{pmatrix}
\sigma_{\alpha} & 0 \\
0 & \sigma_{\beta} \\
\end{pmatrix} \
\textbf{R} \begin{pmatrix}
\sigma_{\alpha} & 0 \\
0 & \sigma_{\beta} \\
\end{pmatrix}} \\
\color{steelblue}{\alpha} \ &\color{steelblue}{\sim \mathrm{Normal} (0, 10)} \\
\color{steelblue}{\beta} \ &\color{steelblue}{\sim \mathrm{Normal} (0, 10)} \\
\color{steelblue}{\sigma_{\alpha}} \ &\color{steelblue}{\sim \mathrm{HalfCauchy} (0, 2)} \\
\color{steelblue}{\sigma_{\beta}} \ &\color{steelblue}{\sim \mathrm{HalfCauchy} (0, 2)} \\
\color{steelblue}{\sigma} \ &\color{steelblue}{\sim \mathrm{HalfCauchy} (0, 2)} \\
\color{steelblue}{\textbf{R}} \ &\color{steelblue}{\sim \mathrm{LKJ}(2)} \\
\end{align}
$$

$\textbf{S}$ is defined by factoring $\sigma_{\alpha}$, $\sigma_{\beta}$, and the correlation matrix $\textbf{R}$. The next lines of the model simply define priors for constant effects. The last line specifies the prior for $\textbf{R}$.

## LKJ prior

Prior proposed by @lewandowski2009. A single parameter $\zeta$ (zeta) specifies the concentration of the distribution of the correlation coefficient. The $\mathrm{LKJ}(2)$ prior defines an uninformative prior for $\rho$ (rho) which is sceptical of extreme correlations (i.e., values close to $-1$ or $1$).

```{r, echo = FALSE, fig.width = 14, fig.height = 7, cache = TRUE}
library(ggdist)

expand.grid(eta = c(0.5, 2, 5, 10), K = c(2, 3, 4, 5) ) %>%
  ggplot(
      aes(
          y = ordered(eta), dist = "lkjcorr_marginal",
          arg1 = K, arg2 = eta, fill = as.factor(eta)
          )
      ) +
  stat_dist_slab(p_limits = c(0, 1), alpha = 0.8) +
  facet_grid(~paste0(K, "x", K) ) +
  labs(x = expression(rho), y = "Probability density (per prior)") +
  scale_fill_manual(
      values = c("steelblue", "orangered", "purple", "darkgreen"),
      labels = c(
        expression(paste(zeta, " = ", "0.5") ),
        expression(paste(zeta, " = ", "2") ),
        expression(paste(zeta, " = ", "10") ),
        expression(paste(zeta, " = ", "50") )
        )
      ) +
    theme(
        legend.title = element_blank(),
        legend.text.align = 0,
        legend.background = element_rect(size = 0.5, colour = "black")
        )
```

## Syntax reminders

The `brms` package uses the same syntax as R base functions (like `lm`) or the `lme4` package.

```{r eval = FALSE, echo = TRUE}
Reaction ~ Days + (1 + Days | Subject)
```

The left-hand side defines the dependent variable (or "outcome", i.e., what we are trying to predict).

. . .

The right-hand side defines the predictors. The intercept is usually implied, so the two formulations below are equivalent.

```{r eval = FALSE, echo = TRUE}
Reaction ~ Days + (1 + Days | Subject)
Reaction ~ 1 + Days + (1 + Days | Subject)
```

## Syntax reminders

The first part of the right-hand side of the formula represents the constant effects (fixed effects), whereas the second part (between parentheses) represents varying effects (random effects).

```{r eval = FALSE, echo = TRUE}
Reaction ~ 1 + Days + (1 | Subject)
Reaction ~ 1 + Days + (1 + Days | Subject)
```

The first model above contains only an intercept variable, which varies by `Subject`. The second model also contains a
variable intercept, but also a variable slope for the effect of `Days`.

## Syntax reminders

When including several varying effects (e.g., an intercept and a slope), `brms` assumes that we also want to estimate the
correlation between these two effects. Otherwise, we can remove this correlation (i.e., set it to 0) using `||`.

```{r eval = FALSE, echo = TRUE}
Reaction ~ Days + (1 + Days || Subject)
```

. . .

Previous models assumed a Gaussian generative model. This assumption can be changed easily by specifying the desired function via the `family` argument.

```{r eval = FALSE, echo = TRUE}
brm(formula = Reaction ~ 1 + Days + (1 + Days | Subject), family = lognormal() )
```

## Implementation of our model via brms

We specify an intercept and a slope (for the `afternoon` effect) which vary by `cafe`.

```{r eval = TRUE, echo = TRUE, results = "hide"}
mod5 <- brm(
  formula = wait ~ 1 + afternoon + (1 + afternoon | cafe),
  prior = c(
    prior(normal(0, 10), class = Intercept),
    prior(normal(0, 10), class = b),
    prior(cauchy(0, 2), class = sigma),
    prior(cauchy(0, 2), class = sd)
    ),
  data = df,
  warmup = 1000, iter = 5000,
  cores = parallel::detectCores()
  )
```

## Posterior distribution

```{r eval = TRUE, echo = TRUE, fig.width = 9, fig.height = 6}
post <- as_draws_df(x = mod5) # extracting posterior samples
R <- rethinking::rlkjcorr(n = 16000, K = 2, eta = 2) # samples from prior

data.frame(prior = R[, 1, 2], posterior = post$cor_cafe__Intercept__afternoon) %>%
    gather(type, value, prior:posterior) %>%
    ggplot(aes(x = value, color = type, fill = type) ) +
    geom_histogram(position = "identity", alpha = 0.2) +
    labs(x = expression(rho), y = "Number of samples")
```

## Two-dimensional shrinkage

```{r eval = TRUE, echo = FALSE, fig.width = 12, fig.height = 8}
a1 <- sapply(1:20, function(i) mean(df$wait[df$cafe == i & df$afternoon == 0]) )
b1 <- sapply(1:20, function(i) mean(df$wait[df$cafe == i & df$afternoon == 1]) ) - a1

no_pooling <-
  data.frame(Intercept = a1, afternoon = b1) %>%
  mutate(model = "no pooling")

partial_pooling <-
  data.frame(coef(mod5)$cafe[, 1, 1:2]) %>%
  mutate(model = "partial pooling")

shrinkage <- bind_rows(no_pooling, partial_pooling)

mu <- c(mean(post$b_Intercept), mean(post$b_afternoon) )
rho <- mean(post$cor_cafe__Intercept__afternoon)
sda <- mean(post$sd_cafe__Intercept)
sdb <- mean(post$sd_cafe__afternoon)
cov_ab <- sda * sdb * rho
sigma <- matrix(c(sda^2, cov_ab, cov_ab, sdb^2), ncol = 2)

##############################################################################
# Helper function to make ellipse, credits to Tristan Mahr                   #
# https://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/ #
##############################################################################

library(ellipse)

make_ellipse <- function(cov_mat, center, level) {
    
    ellipse(cov_mat, centre = center, level = level) %>%
        as.data.frame() %>%
        add_column(level = level)
    
}

levels <- c(.1, .3, .5, .7)

df_ellipse <-
    levels %>%
    purrr::map_df(~ make_ellipse(sigma, mu, level = .x) ) %>% 
    rename(Intercept = x, afternoon = y)

shrinkage %>%
    mutate(id = rep(1:20, 2) ) %>%
    ggplot(aes(x = Intercept, y = afternoon, color = model) ) +
    scale_color_manual(values = wesanderson::wes_palette(n = 2, name = "Chevalier1") ) +
    geom_point(size = 5, show.legend = FALSE) +
    # connecting lines
    geom_path(
        aes(group = id, color = NULL),
        arrow = arrow(length = unit(.015, "npc"), type = "closed"), 
        show.legend = FALSE
        ) +
    # ellipses
    geom_path(
        aes(group = level, color = NULL),
        data = df_ellipse,
        linetype = "dashed", color = "grey40", alpha = 0.8
        ) +
    labs(x = "Intercept", y = "Slope")
```

## Model comparaison

We compare the first model (complete pooling model), the third model (partial pooling model), and the last model (with varying intercept and slope).

```{r eval = TRUE, echo = TRUE}
mod5 <- add_criterion(mod5, "waic")
w <- loo_compare(mod1, mod2, mod3, mod5, criterion = "waic")
print(w, simplify = FALSE)
model_weights(mod1, mod2, mod3, mod5, weights = "waic")
```

## Model comparaison

The estimate of the average waiting time is more uncertain when we takes into account new sources of error. However, the overall error of the model (i.e., what is not explained), the residual variation $\sigma$, decreases...

```{r eval = TRUE, echo = TRUE, warning = FALSE}
posterior_summary(mod1, pars = c("^b", "sigma") )
posterior_summary(mod3, pars = c("^b", "sigma") )
posterior_summary(mod5, pars = c("^b", "sigma") )
```

## Conclusions

Multilevel models (or "mixed-effects models") are natural extensions of classical (single-level) regression models, where classical parameters are themselves assigned "models", governed by hyper-parameters.

. . .

This extension makes it possible to make more precise predictions by taking into account the variability related to groups or structures (clusters) present in the data. In other words, by modelling the populations from which the varying effects are drawn (e.g., the population of participants or stimuli).

A single-level regression model is equivalent to a multilevel model where the variability of varying effects would be fixed at $0$.

. . .

The Bayesian framework allows a natural interpretation of distributions from which the varying effects come. Indeed, these distributions can be interpreted as prior distributions, whose parameters are estimated from the
data.

## Practical work - sleepstudy

```{r eval = TRUE, echo = TRUE}
library(lme4)
data(sleepstudy)
head(sleepstudy, 20)
```

## Practical work - sleepstudy

```{r eval = TRUE, echo = TRUE, fig.width = 12, fig.height = 6}
sleepstudy %>%
    ggplot(aes(x = Days, y = Reaction) ) +
    geom_smooth(method = "lm", colour = "black") +
    geom_point() +
    facet_wrap(~Subject, nrow = 2) +
    scale_x_continuous(breaks = c(0, 2, 4, 6, 8) )
```

## Practical work - sleepstudy

::: nonincremental
It's up to you to build the mathematical models and `brms` models corresponding to the following models:

- A model with only the fixed effect of `Days`.
- A model with the fixed effect of `Days` + a random effect of `Subject` (varying intercept).
- A model with the fixed effect of `Days` + a random effect of `Subject` (varying intercept + varying slope for `Days`).

Then, compare these models using model comparison tools and conclude.
:::

## Proposed solution

```{r eval = TRUE, echo = TRUE}
fmod0 <- lm(Reaction ~ Days, sleepstudy)
fmod1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
fmod2 <- lmer(Reaction ~ Days + (1 + Days | Subject), sleepstudy)

anova(fmod1, fmod2)
```

## Proposed solution

```{r eval = TRUE, echo = TRUE, results = "hide"}
mod6 <- brm(
  Reaction ~ 1 + Days,
  prior = c(
    prior(normal(200, 100), class = Intercept),
    prior(normal(0, 10), class = b),
    prior(cauchy(0, 10), class = sigma)
    ),
  data = sleepstudy,
  warmup = 1000, iter = 5000,
  cores = parallel::detectCores()
  )
```

```{r eval = TRUE, echo = TRUE}
posterior_summary(mod6)
```

## Proposed solution

```{r eval = TRUE, echo = TRUE, results = "hide"}
mod7 <- brm(
  Reaction ~ 1 + Days + (1 | Subject),
  prior = c(
    prior(normal(200, 100), class = Intercept),
    prior(normal(0, 10), class = b),
    prior(cauchy(0, 10), class = sigma),
    prior(cauchy(0, 10), class = sd)
    ),
  data = sleepstudy,
  warmup = 1000, iter = 5000,
  cores = parallel::detectCores()
  )
```

```{r eval = TRUE, echo = TRUE}
posterior_summary(mod7, pars = c("^b", "sigma") )
```

## Proposed solution

```{r eval = TRUE, echo = TRUE, results = "hide"}
mod8 <- brm(
  Reaction ~ 1 + Days + (1 + Days | Subject),
  prior = c(
    prior(normal(200, 100), class = Intercept),
    prior(normal(0, 10), class = b),
    prior(cauchy(0, 10), class = sigma),
    prior(cauchy(0, 10), class = sd)
    ),
  data = sleepstudy,
  warmup = 1000, iter = 5000,
  cores = parallel::detectCores()
  )
```

```{r eval = TRUE, echo = TRUE}
posterior_summary(mod8, pars = c("^b", "sigma") )
```

## Proposed solution

```{r eval = TRUE, echo = TRUE}
# computing and storing the WAIC of each model
mod6 <- add_criterion(mod6, "waic")
mod7 <- add_criterion(mod7, "waic")
mod8 <- add_criterion(mod8, "waic")

# comparing the WAICs of these models
w <- loo_compare(mod6, mod7, mod8, criterion = "waic")
print(w, simplify = FALSE)

# computing the relative weight of each model
model_weights(mod6, mod7, mod8, weights = "waic")
```

# Scientific and cognitive modelling

## What's a model?

...

## What's in a model?

Assumptions (not arbitrary!)...

## What is a model good for?

"One of the most basic problem in scientific inference is the so-called inverse problem: How to figure out causes from observations. It is a problem, because many different causes can produce the same evidence. So while it can be easy to go forward from a known cause to predicted observations, it can very hard to go backwards from observation to cause" (McElreath, 2020)...

## Geometric people

...

## Evidence accumulation models

...

## Drift diffusion model

...

## Bayesian workflow [@gelman2020]

```{r echo = FALSE, out.width = "66%"}
knitr::include_graphics("figures/bayes_workflow_1.png")
```

## Bayesian workflow [@gelman2020]

```{r echo = FALSE, out.width = "50%"}
knitr::include_graphics("figures/bayes_workflow_2.png")
```

## Conclusions

Bayesian inference is a general approach to parameter estimation. This approach uses probability theory to quantify the uncertainty with respect to the value of the parameters of statistical models.

. . .

These models are composed of different blocks (e.g., likelihood function, priors, linear or non-linear model) which are
modifiable as desired. What is classically called "conditions of application" are simply the consequences of modelling choices. In other words, the user defines (and does not suffer) the conditions of application.

. . .

We have seen that the linear regression model provides a very flexible architecture which makes possible to describe, via the modification of the likelihood function and via the introduction of link functions, complex (e.g., non-linear) relationships between outcomes and predictors. These models can gain in precision by taking into account the variability and structures present in the data (cf. multilevel models).

## Conclusions

The `brms` package is a real Swiss army knife of Bayesian statistics in `R`. It allows you to fit almost any
type of regression model. This includes all models that we have seen, but also many others. Among others,
multivariate models (i.e., models with several outcomes), "distributional" models (e.g., to predict variance differences),
[generalized additive models](https://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/), [Gaussian processes](https://rdrr.io/cran/brms/man/gp.html) (Gaussian processes), models from [signal detection theory](https://mvuorre.github.io/posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/),
[mixture models](https://www.martinmodrak.cz/2021/04/01/using-brms-to-model-reaction-times-contaminated-with-errors/),
[drift diffusion models](http://singmann.org/wiener-model-analysis-with-brms-part-i/), [non-linear models](https://paul-buerkner.github.io/brms/articles/brms_nonlinear.html)...

Do not hesitate to contact me for more information on these models or if you have any questions about your own data. You can also contact the creator of the `brms` package, who is very active online (see [his site](https://paul-buerkner.github.io/about/)). See also the [Stan forum](https://discourse.mc-stan.org).

## References {.refs}
